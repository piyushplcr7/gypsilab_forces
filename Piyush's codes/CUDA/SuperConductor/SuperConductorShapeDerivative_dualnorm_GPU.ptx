//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31057947
// Cuda compilation tools, release 11.6, V11.6.124
// Based on NVVM 7.0.1
//

.version 7.6
.target sm_52
.address_size 64

	// .globl	_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_
.extern .func  (.param .b64 func_retval0) malloc
(
	.param .b64 malloc_param_0
)
;
.extern .func free
(
	.param .b64 free_param_0
)
;
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
// _ZZ22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_E21localShapeDerivatives has been demoted
.global .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};
.global .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_(
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_0,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_1,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_2,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_3,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_4,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_5,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_6,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_7,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_8,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_9,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_10,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_11,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_12,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_13,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_14,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_15,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_16,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_17,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_18,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_19,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_20,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_21,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_22,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_23,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_24,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_25,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_26,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_27,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_28,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_29,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_30,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_31,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_32,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_33,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_34,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_35,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_36,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_37,
	.param .u32 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_38,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_39,
	.param .u64 _Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_40
)
{
	.local .align 16 .b8 	__local_depot0[288];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<117>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<397>;
	.reg .f64 	%fd<905>;
	.reg .b64 	%rd<434>;
	// demoted variable
	.shared .align 8 .b8 _ZZ22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_E21localShapeDerivatives[20736];

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r136, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_4];
	ld.param.u32 	%r137, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_5];
	ld.param.u64 	%rd90, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_8];
	ld.param.u64 	%rd100, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_24];
	ld.param.u32 	%r142, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_38];
	ld.param.u64 	%rd108, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_39];
	ld.param.u64 	%rd109, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_40];
	add.u64 	%rd110, %SP, 8;
	add.u64 	%rd1, %SPL, 8;
	add.u64 	%rd119, %SP, 0;
	add.u64 	%rd10, %SPL, 0;
	add.u64 	%rd17, %SPL, 32;
	add.u64 	%rd18, %SPL, 56;
	add.u64 	%rd19, %SPL, 68;
	add.u64 	%rd20, %SPL, 80;
	add.u64 	%rd21, %SPL, 92;
	add.u64 	%rd22, %SPL, 104;
	add.u64 	%rd23, %SPL, 116;
	add.u64 	%rd24, %SPL, 208;
	mov.u32 	%r1, %tid.x;
	setp.lt.s32 	%p4, %r142, 1;
	@%p4 bra 	$L__BB0_7;

	mul.lo.s32 	%r2, %r1, %r142;
	and.b32  	%r360, %r142, 3;
	add.s32 	%r144, %r142, -1;
	setp.lt.u32 	%p5, %r144, 3;
	mov.u32 	%r358, 0;
	@%p5 bra 	$L__BB0_4;

	sub.s32 	%r357, %r142, %r360;
	mov.u64 	%rd134, 0;

$L__BB0_3:
	add.s32 	%r146, %r358, %r2;
	shl.b32 	%r147, %r146, 3;
	mov.u32 	%r148, _ZZ22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_E21localShapeDerivatives;
	add.s32 	%r149, %r148, %r147;
	st.shared.u64 	[%r149], %rd134;
	st.shared.u64 	[%r149+8], %rd134;
	st.shared.u64 	[%r149+16], %rd134;
	st.shared.u64 	[%r149+24], %rd134;
	add.s32 	%r358, %r358, 4;
	add.s32 	%r357, %r357, -4;
	setp.ne.s32 	%p6, %r357, 0;
	@%p6 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s32 	%p7, %r360, 0;
	@%p7 bra 	$L__BB0_7;

	add.s32 	%r150, %r358, %r2;
	shl.b32 	%r151, %r150, 3;
	mov.u32 	%r152, _ZZ22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_E21localShapeDerivatives;
	add.s32 	%r359, %r152, %r151;
	mov.u64 	%rd135, 0;

$L__BB0_6:
	.pragma "nounroll";
	st.shared.u64 	[%r359], %rd135;
	add.s32 	%r359, %r359, 8;
	add.s32 	%r360, %r360, -1;
	setp.ne.s32 	%p8, %r360, 0;
	@%p8 bra 	$L__BB0_6;

$L__BB0_7:
	cvt.rn.f64.s32 	%fd237, %r136;
	cvt.rn.f64.s32 	%fd238, %r137;
	div.rn.f64 	%fd239, %fd237, %fd238;
	cvt.rpi.f64.f64 	%fd240, %fd239;
	cvt.rzi.s32.f64 	%r15, %fd240;
	setp.lt.s32 	%p9, %r15, 1;
	@%p9 bra 	$L__BB0_174;

	cvta.to.global.u64 	%rd25, %rd109;
	cvta.to.global.u64 	%rd26, %rd108;
	cvta.to.global.u64 	%rd27, %rd100;
	cvta.to.global.u64 	%rd28, %rd90;
	mov.u32 	%r154, %ntid.x;
	mov.u32 	%r155, %ctaid.x;
	mad.lo.s32 	%r156, %r155, %r154, %r1;
	mul.lo.s32 	%r16, %r15, %r156;
	mul.lo.s32 	%r17, %r1, %r142;
	mov.u32 	%r361, 0;

$L__BB0_9:
	add.s32 	%r19, %r361, %r16;
	mov.u64 	%rd139, 0;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd139;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 0
	mov.u64 	%rd140, 48;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd32, [retval0+0];
	} // callseq 1
	setp.ne.s64 	%p10, %rd32, 0;
	@%p10 bra 	$L__BB0_11;

	mov.u64 	%rd141, -1;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd141;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd142, [retval0+0];
	} // callseq 2

$L__BB0_11:
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd139;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 3
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd33, [retval0+0];
	} // callseq 4
	setp.ne.s64 	%p11, %rd33, 0;
	@%p11 bra 	$L__BB0_13;

	mov.u64 	%rd145, -1;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd146, [retval0+0];
	} // callseq 5

$L__BB0_13:
	ld.param.u32 	%r345, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_4];
	setp.lt.s32 	%p12, %r19, %r345;
	@%p12 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_14;

$L__BB0_15:
	ld.param.u64 	%rd405, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_26];
	add.u64 	%rd403, %SPL, 192;
	ld.param.u32 	%r349, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_20];
	ld.param.u32 	%r348, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_17];
	ld.param.u32 	%r347, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_14];
	ld.param.u32 	%r346, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_11];
	ld.param.u64 	%rd402, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_19];
	ld.param.u64 	%rd401, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_16];
	add.u64 	%rd399, %SPL, 160;
	ld.param.u64 	%rd398, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_13];
	ld.param.u64 	%rd397, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_10];
	ld.param.u64 	%rd396, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_18];
	ld.param.u64 	%rd395, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_15];
	add.u64 	%rd393, %SPL, 128;
	ld.param.u64 	%rd392, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_12];
	ld.param.u64 	%rd391, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_9];
	ld.param.u64 	%rd390, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_30];
	ld.param.u64 	%rd389, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_29];
	ld.param.u64 	%rd388, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_25];
	ld.param.u64 	%rd387, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_27];
	ld.param.u64 	%rd386, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_28];
	ld.param.u64 	%rd385, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_7];
	ld.param.u64 	%rd384, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_6];
	mul.wide.s32 	%rd187, %r19, 2;
	add.s64 	%rd147, %rd384, %rd187;
	// begin inline asm
	ld.global.nc.u16 %rs1, [%rd147];
	// end inline asm
	add.s64 	%rd148, %rd385, %rd187;
	// begin inline asm
	ld.global.nc.u16 %rs2, [%rd148];
	// end inline asm
	cvt.u32.u16 	%r169, %rs1;
	mul.wide.u32 	%rd188, %r169, 8;
	add.s64 	%rd149, %rd386, %rd188;
	// begin inline asm
	ld.global.nc.f64 %fd241, [%rd149];
	// end inline asm
	cvt.u32.u16 	%r170, %rs2;
	mul.wide.u32 	%rd189, %r170, 8;
	add.s64 	%rd150, %rd386, %rd189;
	// begin inline asm
	ld.global.nc.f64 %fd242, [%rd150];
	// end inline asm
	mul.wide.u16 	%r171, %rs1, 3;
	mul.wide.u32 	%rd190, %r171, 8;
	add.s64 	%rd151, %rd387, %rd190;
	// begin inline asm
	ld.global.nc.f64 %fd243, [%rd151];
	// end inline asm
	add.s64 	%rd152, %rd151, 8;
	// begin inline asm
	ld.global.nc.f64 %fd244, [%rd152];
	// end inline asm
	add.s64 	%rd153, %rd151, 16;
	// begin inline asm
	ld.global.nc.f64 %fd245, [%rd153];
	// end inline asm
	mul.wide.u16 	%r172, %rs2, 3;
	mul.wide.u32 	%rd191, %r172, 8;
	add.s64 	%rd154, %rd387, %rd191;
	// begin inline asm
	ld.global.nc.f64 %fd246, [%rd154];
	// end inline asm
	add.s64 	%rd155, %rd154, 8;
	// begin inline asm
	ld.global.nc.f64 %fd247, [%rd155];
	// end inline asm
	add.s64 	%rd156, %rd154, 16;
	// begin inline asm
	ld.global.nc.f64 %fd248, [%rd156];
	// end inline asm
	mul.wide.s32 	%rd192, %r171, 4;
	add.s64 	%rd157, %rd388, %rd192;
	// begin inline asm
	ld.global.nc.s32 %r157, [%rd157];
	// end inline asm
	add.s64 	%rd158, %rd157, 4;
	// begin inline asm
	ld.global.nc.s32 %r158, [%rd158];
	// end inline asm
	add.s64 	%rd159, %rd157, 8;
	// begin inline asm
	ld.global.nc.s32 %r159, [%rd159];
	// end inline asm
	mul.wide.s32 	%rd193, %r172, 4;
	add.s64 	%rd160, %rd388, %rd193;
	// begin inline asm
	ld.global.nc.s32 %r160, [%rd160];
	// end inline asm
	add.s64 	%rd161, %rd160, 4;
	// begin inline asm
	ld.global.nc.s32 %r161, [%rd161];
	// end inline asm
	add.s64 	%rd162, %rd160, 8;
	// begin inline asm
	ld.global.nc.s32 %r162, [%rd162];
	// end inline asm
	st.local.u32 	[%rd18], %r157;
	st.local.u32 	[%rd18+4], %r158;
	st.local.u32 	[%rd18+8], %r159;
	st.local.u32 	[%rd19], %r160;
	st.local.u32 	[%rd19+4], %r161;
	st.local.u32 	[%rd19+8], %r162;
	add.s64 	%rd163, %rd389, %rd192;
	// begin inline asm
	ld.global.nc.s32 %r163, [%rd163];
	// end inline asm
	st.local.u32 	[%rd20], %r163;
	add.s64 	%rd164, %rd163, 4;
	// begin inline asm
	ld.global.nc.s32 %r164, [%rd164];
	// end inline asm
	st.local.u32 	[%rd20+4], %r164;
	add.s64 	%rd165, %rd163, 8;
	// begin inline asm
	ld.global.nc.s32 %r165, [%rd165];
	// end inline asm
	st.local.u32 	[%rd20+8], %r165;
	add.s64 	%rd166, %rd390, %rd193;
	// begin inline asm
	ld.global.nc.s32 %r166, [%rd166];
	// end inline asm
	st.local.u32 	[%rd21], %r166;
	add.s64 	%rd167, %rd166, 4;
	// begin inline asm
	ld.global.nc.s32 %r167, [%rd167];
	// end inline asm
	st.local.u32 	[%rd21+4], %r167;
	add.s64 	%rd168, %rd166, 8;
	// begin inline asm
	ld.global.nc.s32 %r168, [%rd168];
	// end inline asm
	st.local.u32 	[%rd21+8], %r168;
	st.local.v2.u64 	[%rd393], {%rd391, %rd392};
	st.local.v2.u64 	[%rd393+16], {%rd395, %rd396};
	st.local.v2.u64 	[%rd399], {%rd397, %rd398};
	st.local.v2.u64 	[%rd399+16], {%rd401, %rd402};
	st.local.v4.u32 	[%rd403], {%r346, %r347, %r348, %r349};
	mul.wide.s32 	%rd195, %r19, 4;
	add.s64 	%rd196, %rd28, %rd195;
	ld.global.u32 	%r173, [%rd196];
	mul.wide.s32 	%rd197, %r173, 8;
	add.s64 	%rd198, %rd393, %rd197;
	ld.local.u64 	%rd35, [%rd198];
	add.s64 	%rd199, %rd399, %rd197;
	ld.local.u64 	%rd36, [%rd199];
	mul.wide.s32 	%rd200, %r173, 4;
	add.s64 	%rd201, %rd403, %rd200;
	ld.local.u32 	%r20, [%rd201];
	mul.lo.s32 	%r174, %r19, 3;
	mul.wide.s32 	%rd202, %r174, 4;
	add.s64 	%rd203, %rd26, %rd202;
	ld.global.u32 	%r175, [%rd203];
	st.local.u32 	[%rd22], %r175;
	add.s64 	%rd204, %rd25, %rd202;
	ld.global.u32 	%r176, [%rd204];
	st.local.u32 	[%rd23], %r176;
	mul.wide.s32 	%rd205, %r175, 4;
	add.s64 	%rd206, %rd18, %rd205;
	ld.local.u32 	%r177, [%rd206];
	mul.wide.s32 	%rd207, %r176, 4;
	add.s64 	%rd208, %rd19, %rd207;
	ld.local.u32 	%r178, [%rd208];
	ld.global.u32 	%r179, [%rd203+4];
	st.local.u32 	[%rd22+4], %r179;
	ld.global.u32 	%r180, [%rd204+4];
	st.local.u32 	[%rd23+4], %r180;
	mul.wide.s32 	%rd209, %r179, 4;
	add.s64 	%rd210, %rd18, %rd209;
	ld.local.u32 	%r181, [%rd210];
	mul.wide.s32 	%rd211, %r180, 4;
	add.s64 	%rd212, %rd19, %rd211;
	ld.local.u32 	%r182, [%rd212];
	ld.global.u32 	%r183, [%rd203+8];
	st.local.u32 	[%rd22+8], %r183;
	ld.global.u32 	%r184, [%rd204+8];
	st.local.u32 	[%rd23+8], %r184;
	mul.wide.s32 	%rd213, %r183, 4;
	add.s64 	%rd214, %rd18, %rd213;
	ld.local.u32 	%r185, [%rd214];
	mul.wide.s32 	%rd215, %r184, 4;
	add.s64 	%rd216, %rd19, %rd215;
	ld.local.u32 	%r186, [%rd216];
	mul.lo.s32 	%r187, %r177, 3;
	mul.wide.s32 	%rd217, %r187, 8;
	add.s64 	%rd169, %rd405, %rd217;
	// begin inline asm
	ld.global.nc.f64 %fd249, [%rd169];
	// end inline asm
	add.s64 	%rd170, %rd169, 8;
	// begin inline asm
	ld.global.nc.f64 %fd250, [%rd170];
	// end inline asm
	add.s64 	%rd171, %rd169, 16;
	// begin inline asm
	ld.global.nc.f64 %fd251, [%rd171];
	// end inline asm
	mul.lo.s32 	%r188, %r181, 3;
	mul.wide.s32 	%rd218, %r188, 8;
	add.s64 	%rd172, %rd405, %rd218;
	// begin inline asm
	ld.global.nc.f64 %fd252, [%rd172];
	// end inline asm
	add.s64 	%rd173, %rd172, 8;
	// begin inline asm
	ld.global.nc.f64 %fd253, [%rd173];
	// end inline asm
	add.s64 	%rd174, %rd172, 16;
	// begin inline asm
	ld.global.nc.f64 %fd254, [%rd174];
	// end inline asm
	mul.lo.s32 	%r189, %r185, 3;
	mul.wide.s32 	%rd219, %r189, 8;
	add.s64 	%rd175, %rd405, %rd219;
	// begin inline asm
	ld.global.nc.f64 %fd255, [%rd175];
	// end inline asm
	add.s64 	%rd176, %rd175, 8;
	// begin inline asm
	ld.global.nc.f64 %fd256, [%rd176];
	// end inline asm
	add.s64 	%rd177, %rd175, 16;
	// begin inline asm
	ld.global.nc.f64 %fd257, [%rd177];
	// end inline asm
	mul.lo.s32 	%r190, %r178, 3;
	mul.wide.s32 	%rd220, %r190, 8;
	add.s64 	%rd178, %rd405, %rd220;
	// begin inline asm
	ld.global.nc.f64 %fd258, [%rd178];
	// end inline asm
	add.s64 	%rd179, %rd178, 8;
	// begin inline asm
	ld.global.nc.f64 %fd259, [%rd179];
	// end inline asm
	add.s64 	%rd180, %rd178, 16;
	// begin inline asm
	ld.global.nc.f64 %fd260, [%rd180];
	// end inline asm
	mul.lo.s32 	%r191, %r182, 3;
	mul.wide.s32 	%rd221, %r191, 8;
	add.s64 	%rd181, %rd405, %rd221;
	// begin inline asm
	ld.global.nc.f64 %fd261, [%rd181];
	// end inline asm
	add.s64 	%rd182, %rd181, 8;
	// begin inline asm
	ld.global.nc.f64 %fd262, [%rd182];
	// end inline asm
	add.s64 	%rd183, %rd181, 16;
	// begin inline asm
	ld.global.nc.f64 %fd263, [%rd183];
	// end inline asm
	mul.lo.s32 	%r192, %r186, 3;
	mul.wide.s32 	%rd222, %r192, 8;
	add.s64 	%rd184, %rd405, %rd222;
	// begin inline asm
	ld.global.nc.f64 %fd264, [%rd184];
	// end inline asm
	add.s64 	%rd185, %rd184, 8;
	// begin inline asm
	ld.global.nc.f64 %fd265, [%rd185];
	// end inline asm
	add.s64 	%rd186, %rd184, 16;
	// begin inline asm
	ld.global.nc.f64 %fd266, [%rd186];
	// end inline asm
	sub.f64 	%fd267, %fd252, %fd249;
	st.f64 	[%rd32], %fd267;
	sub.f64 	%fd268, %fd253, %fd250;
	st.f64 	[%rd32+8], %fd268;
	sub.f64 	%fd269, %fd254, %fd251;
	st.f64 	[%rd32+16], %fd269;
	sub.f64 	%fd270, %fd255, %fd249;
	st.f64 	[%rd32+24], %fd270;
	sub.f64 	%fd271, %fd256, %fd250;
	mov.u64 	%rd223, 32;
	st.f64 	[%rd32+32], %fd271;
	sub.f64 	%fd272, %fd257, %fd251;
	st.f64 	[%rd32+40], %fd272;
	sub.f64 	%fd273, %fd261, %fd258;
	st.f64 	[%rd33], %fd273;
	sub.f64 	%fd274, %fd262, %fd259;
	st.f64 	[%rd33+8], %fd274;
	sub.f64 	%fd275, %fd263, %fd260;
	st.f64 	[%rd33+16], %fd275;
	sub.f64 	%fd276, %fd264, %fd258;
	st.f64 	[%rd33+24], %fd276;
	sub.f64 	%fd277, %fd265, %fd259;
	st.f64 	[%rd33+32], %fd277;
	sub.f64 	%fd278, %fd266, %fd260;
	st.f64 	[%rd33+40], %fd278;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd139;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 8
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd38, [retval0+0];
	} // callseq 9
	setp.ne.s64 	%p13, %rd38, 0;
	@%p13 bra 	$L__BB0_17;

	mov.u64 	%rd224, -1;
	{ // callseq 10, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd224;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd225, [retval0+0];
	} // callseq 10

$L__BB0_17:
	mov.u64 	%rd425, %rd139;

$L__BB0_18:
	shl.b64 	%rd40, %rd425, 1;
	mul.lo.s64 	%rd228, %rd425, 24;
	add.s64 	%rd41, %rd32, %rd228;
	mov.u64 	%rd426, %rd139;

$L__BB0_19:
	add.s64 	%rd229, %rd426, %rd40;
	shl.b64 	%rd230, %rd229, 3;
	add.s64 	%rd231, %rd38, %rd230;
	mul.lo.s64 	%rd232, %rd426, 24;
	add.s64 	%rd233, %rd32, %rd232;
	ld.f64 	%fd279, [%rd41];
	ld.f64 	%fd280, [%rd233];
	ld.f64 	%fd281, [%rd41+8];
	ld.f64 	%fd282, [%rd233+8];
	mul.f64 	%fd283, %fd282, %fd281;
	fma.rn.f64 	%fd284, %fd280, %fd279, %fd283;
	ld.f64 	%fd285, [%rd41+16];
	ld.f64 	%fd286, [%rd233+16];
	fma.rn.f64 	%fd287, %fd286, %fd285, %fd284;
	st.f64 	[%rd231], %fd287;
	add.s64 	%rd426, %rd426, 1;
	setp.lt.u64 	%p14, %rd426, 2;
	@%p14 bra 	$L__BB0_19;

	add.s64 	%rd425, %rd425, 1;
	setp.lt.u64 	%p15, %rd425, 2;
	@%p15 bra 	$L__BB0_18;

	mov.u64 	%rd234, 0;
	{ // callseq 11, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd234;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 11
	{ // callseq 12, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd45, [retval0+0];
	} // callseq 12
	setp.ne.s64 	%p16, %rd45, 0;
	@%p16 bra 	$L__BB0_23;

	mov.u64 	%rd236, -1;
	{ // callseq 13, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd236;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd237, [retval0+0];
	} // callseq 13

$L__BB0_23:
	mov.u64 	%rd427, %rd234;

$L__BB0_24:
	shl.b64 	%rd47, %rd427, 1;
	mul.lo.s64 	%rd240, %rd427, 24;
	add.s64 	%rd48, %rd33, %rd240;
	mov.u64 	%rd428, %rd234;

$L__BB0_25:
	add.s64 	%rd241, %rd428, %rd47;
	shl.b64 	%rd242, %rd241, 3;
	add.s64 	%rd243, %rd45, %rd242;
	mul.lo.s64 	%rd244, %rd428, 24;
	add.s64 	%rd245, %rd33, %rd244;
	ld.f64 	%fd288, [%rd48];
	ld.f64 	%fd289, [%rd245];
	ld.f64 	%fd290, [%rd48+8];
	ld.f64 	%fd291, [%rd245+8];
	mul.f64 	%fd292, %fd291, %fd290;
	fma.rn.f64 	%fd293, %fd289, %fd288, %fd292;
	ld.f64 	%fd294, [%rd48+16];
	ld.f64 	%fd295, [%rd245+16];
	fma.rn.f64 	%fd296, %fd295, %fd294, %fd293;
	st.f64 	[%rd243], %fd296;
	add.s64 	%rd428, %rd428, 1;
	setp.lt.u64 	%p17, %rd428, 2;
	@%p17 bra 	$L__BB0_25;

	add.s64 	%rd427, %rd427, 1;
	setp.lt.u64 	%p18, %rd427, 2;
	@%p18 bra 	$L__BB0_24;

	ld.f64 	%fd297, [%rd38+24];
	mov.u64 	%rd246, 0;
	ld.f64 	%fd298, [%rd38];
	mul.f64 	%fd299, %fd298, %fd297;
	ld.f64 	%fd300, [%rd38+8];
	ld.f64 	%fd301, [%rd38+16];
	mul.f64 	%fd302, %fd301, %fd300;
	sub.f64 	%fd7, %fd299, %fd302;
	ld.f64 	%fd303, [%rd45+24];
	ld.f64 	%fd304, [%rd45];
	mul.f64 	%fd305, %fd304, %fd303;
	ld.f64 	%fd306, [%rd45+8];
	ld.f64 	%fd307, [%rd45+16];
	mul.f64 	%fd308, %fd307, %fd306;
	sub.f64 	%fd8, %fd305, %fd308;
	{ // callseq 14, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd246;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 14
	{ // callseq 15, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd54, [retval0+0];
	} // callseq 15
	setp.ne.s64 	%p19, %rd54, 0;
	@%p19 bra 	$L__BB0_29;

	mov.u64 	%rd248, -1;
	{ // callseq 16, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd248;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd249, [retval0+0];
	} // callseq 16

$L__BB0_29:
	ld.f64 	%fd309, [%rd38];
	neg.f64 	%fd310, %fd309;
	st.f64 	[%rd54], %fd310;
	ld.f64 	%fd311, [%rd38+8];
	neg.f64 	%fd312, %fd311;
	st.f64 	[%rd54+8], %fd312;
	ld.f64 	%fd313, [%rd38+16];
	neg.f64 	%fd314, %fd313;
	st.f64 	[%rd54+16], %fd314;
	ld.f64 	%fd315, [%rd38+24];
	neg.f64 	%fd316, %fd315;
	st.f64 	[%rd54+24], %fd316;
	{ // callseq 17, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd246;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 17
	{ // callseq 18, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd56, [retval0+0];
	} // callseq 18
	setp.ne.s64 	%p20, %rd56, 0;
	@%p20 bra 	$L__BB0_31;

	mov.u64 	%rd252, -1;
	{ // callseq 19, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd252;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd253, [retval0+0];
	} // callseq 19

$L__BB0_31:
	ld.f64 	%fd317, [%rd45];
	neg.f64 	%fd318, %fd317;
	st.f64 	[%rd56], %fd318;
	ld.f64 	%fd319, [%rd45+8];
	neg.f64 	%fd320, %fd319;
	st.f64 	[%rd56+8], %fd320;
	ld.f64 	%fd321, [%rd45+16];
	neg.f64 	%fd322, %fd321;
	st.f64 	[%rd56+16], %fd322;
	ld.f64 	%fd323, [%rd45+24];
	neg.f64 	%fd324, %fd323;
	st.f64 	[%rd56+24], %fd324;
	ld.f64 	%fd325, [%rd38+24];
	st.f64 	[%rd54], %fd325;
	ld.f64 	%fd326, [%rd38];
	st.f64 	[%rd54+24], %fd326;
	ld.f64 	%fd327, [%rd45+24];
	st.f64 	[%rd56], %fd327;
	ld.f64 	%fd328, [%rd45];
	st.f64 	[%rd56+24], %fd328;
	ld.f64 	%fd329, [%rd54];
	div.rn.f64 	%fd330, %fd329, %fd7;
	st.f64 	[%rd54], %fd330;
	ld.f64 	%fd331, [%rd54+8];
	div.rn.f64 	%fd332, %fd331, %fd7;
	st.f64 	[%rd54+8], %fd332;
	ld.f64 	%fd333, [%rd54+16];
	div.rn.f64 	%fd334, %fd333, %fd7;
	st.f64 	[%rd54+16], %fd334;
	ld.f64 	%fd335, [%rd54+24];
	div.rn.f64 	%fd336, %fd335, %fd7;
	st.f64 	[%rd54+24], %fd336;
	ld.f64 	%fd337, [%rd56];
	div.rn.f64 	%fd338, %fd337, %fd8;
	st.f64 	[%rd56], %fd338;
	ld.f64 	%fd339, [%rd56+8];
	div.rn.f64 	%fd340, %fd339, %fd8;
	st.f64 	[%rd56+8], %fd340;
	ld.f64 	%fd341, [%rd56+16];
	div.rn.f64 	%fd342, %fd341, %fd8;
	st.f64 	[%rd56+16], %fd342;
	ld.f64 	%fd343, [%rd56+24];
	div.rn.f64 	%fd344, %fd343, %fd8;
	st.f64 	[%rd56+24], %fd344;
	{ // callseq 20, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd246;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 20
	{ // callseq 21, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd57, [retval0+0];
	} // callseq 21
	setp.ne.s64 	%p21, %rd57, 0;
	@%p21 bra 	$L__BB0_33;

	mov.u64 	%rd256, -1;
	{ // callseq 22, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd256;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd257, [retval0+0];
	} // callseq 22

$L__BB0_33:
	mov.u64 	%rd429, %rd246;

$L__BB0_34:
	mul.lo.s64 	%rd59, %rd429, 3;
	shl.b64 	%rd260, %rd429, 4;
	add.s64 	%rd60, %rd54, %rd260;
	mov.u64 	%rd430, %rd246;

$L__BB0_35:
	add.s64 	%rd261, %rd430, %rd59;
	shl.b64 	%rd262, %rd261, 3;
	add.s64 	%rd263, %rd57, %rd262;
	shl.b64 	%rd264, %rd430, 3;
	add.s64 	%rd265, %rd32, %rd264;
	ld.f64 	%fd345, [%rd60];
	ld.f64 	%fd346, [%rd265];
	ld.f64 	%fd347, [%rd60+8];
	ld.f64 	%fd348, [%rd265+24];
	mul.f64 	%fd349, %fd348, %fd347;
	fma.rn.f64 	%fd350, %fd346, %fd345, %fd349;
	st.f64 	[%rd263], %fd350;
	add.s64 	%rd430, %rd430, 1;
	setp.lt.u64 	%p22, %rd430, 3;
	@%p22 bra 	$L__BB0_35;

	add.s64 	%rd429, %rd429, 1;
	setp.lt.u64 	%p23, %rd429, 2;
	@%p23 bra 	$L__BB0_34;

	mov.u64 	%rd266, 0;
	{ // callseq 23, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd266;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 23
	{ // callseq 24, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd64, [retval0+0];
	} // callseq 24
	setp.ne.s64 	%p24, %rd64, 0;
	@%p24 bra 	$L__BB0_39;

	mov.u64 	%rd268, -1;
	{ // callseq 25, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd268;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd269, [retval0+0];
	} // callseq 25

$L__BB0_39:
	mov.u64 	%rd431, %rd266;

$L__BB0_40:
	mul.lo.s64 	%rd66, %rd431, 3;
	shl.b64 	%rd272, %rd431, 4;
	add.s64 	%rd67, %rd56, %rd272;
	mov.u64 	%rd432, %rd266;

$L__BB0_41:
	add.s64 	%rd273, %rd432, %rd66;
	shl.b64 	%rd274, %rd273, 3;
	add.s64 	%rd275, %rd64, %rd274;
	shl.b64 	%rd276, %rd432, 3;
	add.s64 	%rd277, %rd33, %rd276;
	ld.f64 	%fd351, [%rd67];
	ld.f64 	%fd352, [%rd277];
	ld.f64 	%fd353, [%rd67+8];
	ld.f64 	%fd354, [%rd277+24];
	mul.f64 	%fd355, %fd354, %fd353;
	fma.rn.f64 	%fd356, %fd352, %fd351, %fd355;
	st.f64 	[%rd275], %fd356;
	add.s64 	%rd432, %rd432, 1;
	setp.lt.u64 	%p25, %rd432, 3;
	@%p25 bra 	$L__BB0_41;

	add.s64 	%rd431, %rd431, 1;
	setp.lt.u64 	%p26, %rd431, 2;
	@%p26 bra 	$L__BB0_40;

	setp.lt.s32 	%p27, %r20, 1;
	@%p27 bra 	$L__BB0_173;

	mov.u32 	%r362, 0;

$L__BB0_45:
	mov.u32 	%r363, 0;

$L__BB0_46:
	mov.u32 	%r364, 0;
	mul.wide.s32 	%rd280, %r363, 4;
	add.s64 	%rd281, %rd22, %rd280;
	ld.local.u32 	%r197, [%rd281];
	add.s32 	%r198, %r197, 1;
	mul.hi.s32 	%r199, %r198, 1431655766;
	shr.u32 	%r200, %r199, 31;
	add.s32 	%r201, %r199, %r200;
	mul.lo.s32 	%r202, %r201, 3;
	sub.s32 	%r203, %r198, %r202;
	add.s32 	%r204, %r203, 1;
	mul.hi.s32 	%r205, %r204, 1431655766;
	shr.u32 	%r206, %r205, 31;
	add.s32 	%r207, %r205, %r206;
	mul.lo.s32 	%r208, %r207, 3;
	sub.s32 	%r209, %r204, %r208;
	mul.wide.s32 	%rd282, %r203, 4;
	add.s64 	%rd283, %rd18, %rd282;
	mul.wide.s32 	%rd284, %r209, 4;
	add.s64 	%rd285, %rd18, %rd284;
	ld.local.u32 	%r210, [%rd285];
	ld.local.u32 	%r211, [%rd283];
	setp.lt.s32 	%p28, %r211, %r210;
	selp.f64 	%fd9, 0d3FF0000000000000, 0dBFF0000000000000, %p28;
	mul.wide.s32 	%rd286, %r197, 4;
	add.s64 	%rd76, %rd20, %rd286;

$L__BB0_47:
	shr.u32 	%r354, %r363, 1;
	cvt.rn.f64.s32 	%fd840, %r354;
	and.b32  	%r353, %r363, 1;
	cvt.rn.f64.s32 	%fd839, %r353;
	shl.b32 	%r352, %r362, 2;
	mul.wide.s32 	%rd410, %r352, 8;
	add.s64 	%rd409, %rd36, %rd410;
	add.s64 	%rd408, %rd409, 24;
	add.s64 	%rd407, %rd409, 16;
	add.s64 	%rd406, %rd409, 8;
	mul.wide.s32 	%rd291, %r364, 4;
	add.s64 	%rd292, %rd23, %rd291;
	ld.local.u32 	%r24, [%rd292];
	add.s32 	%r214, %r24, 1;
	mul.hi.s32 	%r215, %r214, 1431655766;
	shr.u32 	%r216, %r215, 31;
	add.s32 	%r217, %r215, %r216;
	mul.lo.s32 	%r218, %r217, 3;
	sub.s32 	%r219, %r214, %r218;
	add.s32 	%r220, %r219, 1;
	mul.hi.s32 	%r221, %r220, 1431655766;
	shr.u32 	%r222, %r221, 31;
	add.s32 	%r223, %r221, %r222;
	mul.lo.s32 	%r224, %r223, 3;
	sub.s32 	%r225, %r220, %r224;
	mul.wide.s32 	%rd293, %r219, 4;
	add.s64 	%rd294, %rd19, %rd293;
	mul.wide.s32 	%rd295, %r225, 4;
	add.s64 	%rd296, %rd19, %rd295;
	ld.local.u32 	%r226, [%rd296];
	ld.local.u32 	%r227, [%rd294];
	setp.lt.s32 	%p29, %r227, %r226;
	selp.f64 	%fd361, 0d3FF0000000000000, 0dBFF0000000000000, %p29;
	ld.f64 	%fd362, [%rd409];
	sub.f64 	%fd363, %fd362, %fd839;
	ld.f64 	%fd364, [%rd409+8];
	sub.f64 	%fd365, %fd364, %fd840;
	and.b32  	%r228, %r364, 1;
	cvt.rn.f64.s32 	%fd366, %r228;
	ld.f64 	%fd367, [%rd409+16];
	sub.f64 	%fd368, %fd367, %fd366;
	shr.u32 	%r229, %r364, 1;
	cvt.rn.f64.s32 	%fd369, %r229;
	ld.f64 	%fd370, [%rd409+24];
	sub.f64 	%fd371, %fd370, %fd369;
	ld.f64 	%fd372, [%rd32];
	ld.f64 	%fd373, [%rd32+24];
	mul.f64 	%fd374, %fd365, %fd373;
	fma.rn.f64 	%fd375, %fd363, %fd372, %fd374;
	mul.f64 	%fd12, %fd9, %fd375;
	ld.f64 	%fd376, [%rd32+8];
	ld.f64 	%fd377, [%rd32+32];
	mul.f64 	%fd378, %fd365, %fd377;
	fma.rn.f64 	%fd379, %fd363, %fd376, %fd378;
	mul.f64 	%fd13, %fd9, %fd379;
	ld.f64 	%fd380, [%rd32+16];
	ld.f64 	%fd381, [%rd32+40];
	mul.f64 	%fd382, %fd365, %fd381;
	fma.rn.f64 	%fd383, %fd363, %fd380, %fd382;
	mul.f64 	%fd14, %fd9, %fd383;
	ld.f64 	%fd384, [%rd33];
	ld.f64 	%fd385, [%rd33+24];
	mul.f64 	%fd386, %fd371, %fd385;
	fma.rn.f64 	%fd387, %fd368, %fd384, %fd386;
	mul.f64 	%fd15, %fd361, %fd387;
	ld.f64 	%fd388, [%rd33+8];
	ld.f64 	%fd389, [%rd33+32];
	mul.f64 	%fd390, %fd371, %fd389;
	fma.rn.f64 	%fd391, %fd368, %fd388, %fd390;
	mul.f64 	%fd16, %fd361, %fd391;
	ld.f64 	%fd392, [%rd33+16];
	ld.f64 	%fd393, [%rd33+40];
	mul.f64 	%fd394, %fd371, %fd393;
	fma.rn.f64 	%fd395, %fd368, %fd392, %fd394;
	mul.f64 	%fd17, %fd361, %fd395;
	// begin inline asm
	ld.global.nc.f64 %fd357, [%rd409];
	// end inline asm
	// begin inline asm
	ld.global.nc.f64 %fd358, [%rd406];
	// end inline asm
	ld.f64 	%fd396, [%rd32];
	fma.rn.f64 	%fd397, %fd357, %fd396, %fd249;
	ld.f64 	%fd398, [%rd32+24];
	fma.rn.f64 	%fd18, %fd358, %fd398, %fd397;
	ld.f64 	%fd399, [%rd32+8];
	fma.rn.f64 	%fd400, %fd357, %fd399, %fd250;
	ld.f64 	%fd401, [%rd32+32];
	fma.rn.f64 	%fd19, %fd358, %fd401, %fd400;
	ld.f64 	%fd402, [%rd32+16];
	fma.rn.f64 	%fd403, %fd357, %fd402, %fd251;
	ld.f64 	%fd404, [%rd32+40];
	fma.rn.f64 	%fd20, %fd358, %fd404, %fd403;
	// begin inline asm
	ld.global.nc.f64 %fd359, [%rd407];
	// end inline asm
	// begin inline asm
	ld.global.nc.f64 %fd360, [%rd408];
	// end inline asm
	ld.f64 	%fd405, [%rd33];
	fma.rn.f64 	%fd406, %fd359, %fd405, %fd258;
	ld.f64 	%fd407, [%rd33+24];
	fma.rn.f64 	%fd21, %fd360, %fd407, %fd406;
	ld.f64 	%fd408, [%rd33+8];
	fma.rn.f64 	%fd409, %fd359, %fd408, %fd259;
	ld.f64 	%fd410, [%rd33+32];
	fma.rn.f64 	%fd22, %fd360, %fd410, %fd409;
	ld.f64 	%fd411, [%rd33+16];
	fma.rn.f64 	%fd412, %fd359, %fd411, %fd260;
	ld.f64 	%fd413, [%rd33+40];
	fma.rn.f64 	%fd23, %fd360, %fd413, %fd412;
	@%p4 bra 	$L__BB0_170;

	sub.f64 	%fd24, %fd21, %fd18;
	sub.f64 	%fd25, %fd22, %fd19;
	sub.f64 	%fd26, %fd23, %fd20;
	mul.f64 	%fd414, %fd26, %fd26;
	fma.rn.f64 	%fd415, %fd25, %fd25, %fd414;
	fma.rn.f64 	%fd27, %fd24, %fd24, %fd415;
	mul.wide.s32 	%rd297, %r24, 4;
	add.s64 	%rd77, %rd21, %rd297;
	mov.u32 	%r365, 0;

$L__BB0_49:
	mul.wide.s32 	%rd414, %r362, 8;
	add.s64 	%rd413, %rd35, %rd414;
	ld.param.u64 	%rd411, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_37];
	// begin inline asm
	ld.global.nc.f64 %fd416, [%rd413];
	// end inline asm
	shl.b32 	%r235, %r365, 2;
	mul.wide.s32 	%rd303, %r235, 4;
	add.s64 	%rd299, %rd411, %rd303;
	// begin inline asm
	ld.global.nc.s32 %r231, [%rd299];
	// end inline asm
	add.s64 	%rd300, %rd299, 4;
	// begin inline asm
	ld.global.nc.s32 %r232, [%rd300];
	// end inline asm
	add.s64 	%rd301, %rd299, 8;
	// begin inline asm
	ld.global.nc.s32 %r233, [%rd301];
	// end inline asm
	add.s64 	%rd302, %rd299, 12;
	// begin inline asm
	ld.global.nc.s32 %r234, [%rd302];
	// end inline asm
	mov.u64 	%rd304, 0;
	st.local.u64 	[%rd1], %rd304;
	st.local.u64 	[%rd1+8], %rd304;
	st.local.u64 	[%rd1+16], %rd304;
	cvt.rn.f64.s32 	%fd29, %r231;
	mul.f64 	%fd845, %fd18, %fd29;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r236, %temp}, %fd845;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r366}, %fd845;
	}
	and.b32  	%r237, %r366, 2147483647;
	setp.ne.s32 	%p31, %r237, 2146435072;
	setp.ne.s32 	%p32, %r236, 0;
	or.pred  	%p33, %p32, %p31;
	@%p33 bra 	$L__BB0_51;

	mov.f64 	%fd417, 0d0000000000000000;
	mul.rn.f64 	%fd845, %fd845, %fd417;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r366}, %fd845;
	}

$L__BB0_51:
	mul.f64 	%fd418, %fd845, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r367, %fd418;
	st.local.u32 	[%rd10], %r367;
	cvt.rn.f64.s32 	%fd419, %r367;
	neg.f64 	%fd420, %fd419;
	mov.f64 	%fd421, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd422, %fd420, %fd421, %fd845;
	mov.f64 	%fd423, 0d3C91A62633145C00;
	fma.rn.f64 	%fd424, %fd420, %fd423, %fd422;
	mov.f64 	%fd425, 0d397B839A252049C0;
	fma.rn.f64 	%fd846, %fd420, %fd425, %fd424;
	and.b32  	%r238, %r366, 2145386496;
	setp.lt.u32 	%p34, %r238, 1105199104;
	@%p34 bra 	$L__BB0_53;

	{ // callseq 26, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd845;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd846, [retval0+0];
	} // callseq 26
	ld.local.u32 	%r367, [%rd10];

$L__BB0_53:
	add.s32 	%r35, %r367, 1;
	and.b32  	%r239, %r35, 1;
	shl.b32 	%r240, %r35, 3;
	and.b32  	%r241, %r240, 8;
	setp.eq.s32 	%p35, %r239, 0;
	selp.f64 	%fd426, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p35;
	mul.wide.s32 	%rd306, %r241, 8;
	mov.u64 	%rd307, __cudart_sin_cos_coeffs;
	add.s64 	%rd308, %rd307, %rd306;
	ld.global.nc.f64 	%fd427, [%rd308+8];
	mul.rn.f64 	%fd36, %fd846, %fd846;
	fma.rn.f64 	%fd428, %fd426, %fd36, %fd427;
	ld.global.nc.f64 	%fd429, [%rd308+16];
	fma.rn.f64 	%fd430, %fd428, %fd36, %fd429;
	ld.global.nc.f64 	%fd431, [%rd308+24];
	fma.rn.f64 	%fd432, %fd430, %fd36, %fd431;
	ld.global.nc.f64 	%fd433, [%rd308+32];
	fma.rn.f64 	%fd434, %fd432, %fd36, %fd433;
	ld.global.nc.f64 	%fd435, [%rd308+40];
	fma.rn.f64 	%fd436, %fd434, %fd36, %fd435;
	ld.global.nc.f64 	%fd437, [%rd308+48];
	fma.rn.f64 	%fd37, %fd436, %fd36, %fd437;
	fma.rn.f64 	%fd848, %fd37, %fd846, %fd846;
	@%p35 bra 	$L__BB0_55;

	mov.f64 	%fd438, 0d3FF0000000000000;
	fma.rn.f64 	%fd848, %fd37, %fd36, %fd438;

$L__BB0_55:
	and.b32  	%r242, %r35, 2;
	setp.eq.s32 	%p36, %r242, 0;
	@%p36 bra 	$L__BB0_57;

	mov.f64 	%fd439, 0d0000000000000000;
	mov.f64 	%fd440, 0dBFF0000000000000;
	fma.rn.f64 	%fd848, %fd848, %fd440, %fd439;

$L__BB0_57:
	cvt.rn.f64.s32 	%fd43, %r232;
	mul.f64 	%fd849, %fd19, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r243, %temp}, %fd849;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r368}, %fd849;
	}
	and.b32  	%r244, %r368, 2147483647;
	setp.ne.s32 	%p37, %r244, 2146435072;
	setp.ne.s32 	%p38, %r243, 0;
	or.pred  	%p39, %p38, %p37;
	@%p39 bra 	$L__BB0_59;

	mov.f64 	%fd441, 0d0000000000000000;
	mul.rn.f64 	%fd849, %fd849, %fd441;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r368}, %fd849;
	}

$L__BB0_59:
	mul.f64 	%fd442, %fd849, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r369, %fd442;
	st.local.u32 	[%rd10], %r369;
	cvt.rn.f64.s32 	%fd443, %r369;
	neg.f64 	%fd444, %fd443;
	fma.rn.f64 	%fd446, %fd444, %fd421, %fd849;
	fma.rn.f64 	%fd448, %fd444, %fd423, %fd446;
	fma.rn.f64 	%fd850, %fd444, %fd425, %fd448;
	and.b32  	%r245, %r368, 2145386496;
	setp.lt.u32 	%p40, %r245, 1105199104;
	@%p40 bra 	$L__BB0_61;

	{ // callseq 27, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd849;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd850, [retval0+0];
	} // callseq 27
	ld.local.u32 	%r369, [%rd10];

$L__BB0_61:
	add.s32 	%r42, %r369, 1;
	and.b32  	%r246, %r42, 1;
	shl.b32 	%r247, %r42, 3;
	and.b32  	%r248, %r247, 8;
	setp.eq.s32 	%p41, %r246, 0;
	selp.f64 	%fd450, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p41;
	mul.wide.s32 	%rd310, %r248, 8;
	add.s64 	%rd312, %rd307, %rd310;
	ld.global.nc.f64 	%fd451, [%rd312+8];
	mul.rn.f64 	%fd50, %fd850, %fd850;
	fma.rn.f64 	%fd452, %fd450, %fd50, %fd451;
	ld.global.nc.f64 	%fd453, [%rd312+16];
	fma.rn.f64 	%fd454, %fd452, %fd50, %fd453;
	ld.global.nc.f64 	%fd455, [%rd312+24];
	fma.rn.f64 	%fd456, %fd454, %fd50, %fd455;
	ld.global.nc.f64 	%fd457, [%rd312+32];
	fma.rn.f64 	%fd458, %fd456, %fd50, %fd457;
	ld.global.nc.f64 	%fd459, [%rd312+40];
	fma.rn.f64 	%fd460, %fd458, %fd50, %fd459;
	ld.global.nc.f64 	%fd461, [%rd312+48];
	fma.rn.f64 	%fd51, %fd460, %fd50, %fd461;
	fma.rn.f64 	%fd852, %fd51, %fd850, %fd850;
	@%p41 bra 	$L__BB0_63;

	mov.f64 	%fd462, 0d3FF0000000000000;
	fma.rn.f64 	%fd852, %fd51, %fd50, %fd462;

$L__BB0_63:
	and.b32  	%r249, %r42, 2;
	setp.eq.s32 	%p42, %r249, 0;
	@%p42 bra 	$L__BB0_65;

	mov.f64 	%fd463, 0d0000000000000000;
	mov.f64 	%fd464, 0dBFF0000000000000;
	fma.rn.f64 	%fd852, %fd852, %fd464, %fd463;

$L__BB0_65:
	cvt.rn.f64.s32 	%fd57, %r233;
	mul.f64 	%fd853, %fd20, %fd57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r250, %temp}, %fd853;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r370}, %fd853;
	}
	and.b32  	%r251, %r370, 2147483647;
	setp.ne.s32 	%p43, %r251, 2146435072;
	setp.ne.s32 	%p44, %r250, 0;
	or.pred  	%p45, %p44, %p43;
	@%p45 bra 	$L__BB0_67;

	mov.f64 	%fd465, 0d0000000000000000;
	mul.rn.f64 	%fd853, %fd853, %fd465;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r370}, %fd853;
	}

$L__BB0_67:
	mul.f64 	%fd466, %fd853, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r371, %fd466;
	st.local.u32 	[%rd10], %r371;
	cvt.rn.f64.s32 	%fd467, %r371;
	neg.f64 	%fd468, %fd467;
	fma.rn.f64 	%fd470, %fd468, %fd421, %fd853;
	fma.rn.f64 	%fd472, %fd468, %fd423, %fd470;
	fma.rn.f64 	%fd854, %fd468, %fd425, %fd472;
	and.b32  	%r252, %r370, 2145386496;
	setp.lt.u32 	%p46, %r252, 1105199104;
	mul.f64 	%fd62, %fd848, %fd852;
	@%p46 bra 	$L__BB0_69;

	{ // callseq 28, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd853;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd854, [retval0+0];
	} // callseq 28
	ld.local.u32 	%r371, [%rd10];

$L__BB0_69:
	add.s32 	%r49, %r371, 1;
	and.b32  	%r253, %r49, 1;
	shl.b32 	%r254, %r49, 3;
	and.b32  	%r255, %r254, 8;
	setp.eq.s32 	%p47, %r253, 0;
	selp.f64 	%fd474, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p47;
	mul.wide.s32 	%rd314, %r255, 8;
	add.s64 	%rd316, %rd307, %rd314;
	ld.global.nc.f64 	%fd475, [%rd316+8];
	mul.rn.f64 	%fd65, %fd854, %fd854;
	fma.rn.f64 	%fd476, %fd474, %fd65, %fd475;
	ld.global.nc.f64 	%fd477, [%rd316+16];
	fma.rn.f64 	%fd478, %fd476, %fd65, %fd477;
	ld.global.nc.f64 	%fd479, [%rd316+24];
	fma.rn.f64 	%fd480, %fd478, %fd65, %fd479;
	ld.global.nc.f64 	%fd481, [%rd316+32];
	fma.rn.f64 	%fd482, %fd480, %fd65, %fd481;
	ld.global.nc.f64 	%fd483, [%rd316+40];
	fma.rn.f64 	%fd484, %fd482, %fd65, %fd483;
	ld.global.nc.f64 	%fd485, [%rd316+48];
	fma.rn.f64 	%fd66, %fd484, %fd65, %fd485;
	fma.rn.f64 	%fd856, %fd66, %fd854, %fd854;
	@%p47 bra 	$L__BB0_71;

	mov.f64 	%fd486, 0d3FF0000000000000;
	fma.rn.f64 	%fd856, %fd66, %fd65, %fd486;

$L__BB0_71:
	and.b32  	%r256, %r49, 2;
	setp.eq.s32 	%p48, %r256, 0;
	@%p48 bra 	$L__BB0_73;

	mov.f64 	%fd487, 0d0000000000000000;
	mov.f64 	%fd488, 0dBFF0000000000000;
	fma.rn.f64 	%fd856, %fd856, %fd488, %fd487;

$L__BB0_73:
	mov.u64 	%rd417, 0;
	cvt.s64.s32 	%rd80, %r234;
	mul.wide.s32 	%rd317, %r234, 8;
	add.s64 	%rd318, %rd1, %rd317;
	mul.f64 	%fd489, %fd62, %fd856;
	st.local.f64 	[%rd318], %fd489;
	st.local.u64 	[%rd17], %rd417;
	st.local.u64 	[%rd17+8], %rd417;
	st.local.u64 	[%rd17+16], %rd417;
	mul.f64 	%fd857, %fd21, %fd29;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r257, %temp}, %fd857;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r372}, %fd857;
	}
	and.b32  	%r258, %r372, 2147483647;
	setp.ne.s32 	%p49, %r258, 2146435072;
	setp.ne.s32 	%p50, %r257, 0;
	or.pred  	%p51, %p50, %p49;
	@%p51 bra 	$L__BB0_75;

	mov.f64 	%fd490, 0d0000000000000000;
	mul.rn.f64 	%fd857, %fd857, %fd490;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r372}, %fd857;
	}

$L__BB0_75:
	mul.f64 	%fd491, %fd857, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r373, %fd491;
	st.local.u32 	[%rd10], %r373;
	cvt.rn.f64.s32 	%fd492, %r373;
	neg.f64 	%fd493, %fd492;
	fma.rn.f64 	%fd495, %fd493, %fd421, %fd857;
	fma.rn.f64 	%fd497, %fd493, %fd423, %fd495;
	fma.rn.f64 	%fd858, %fd493, %fd425, %fd497;
	and.b32  	%r259, %r372, 2145386496;
	setp.lt.u32 	%p52, %r259, 1105199104;
	@%p52 bra 	$L__BB0_77;

	{ // callseq 29, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd857;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd858, [retval0+0];
	} // callseq 29
	ld.local.u32 	%r373, [%rd10];

$L__BB0_77:
	add.s32 	%r56, %r373, 1;
	and.b32  	%r260, %r56, 1;
	shl.b32 	%r261, %r56, 3;
	and.b32  	%r262, %r261, 8;
	setp.eq.s32 	%p53, %r260, 0;
	selp.f64 	%fd499, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p53;
	mul.wide.s32 	%rd321, %r262, 8;
	add.s64 	%rd323, %rd307, %rd321;
	ld.global.nc.f64 	%fd500, [%rd323+8];
	mul.rn.f64 	%fd78, %fd858, %fd858;
	fma.rn.f64 	%fd501, %fd499, %fd78, %fd500;
	ld.global.nc.f64 	%fd502, [%rd323+16];
	fma.rn.f64 	%fd503, %fd501, %fd78, %fd502;
	ld.global.nc.f64 	%fd504, [%rd323+24];
	fma.rn.f64 	%fd505, %fd503, %fd78, %fd504;
	ld.global.nc.f64 	%fd506, [%rd323+32];
	fma.rn.f64 	%fd507, %fd505, %fd78, %fd506;
	ld.global.nc.f64 	%fd508, [%rd323+40];
	fma.rn.f64 	%fd509, %fd507, %fd78, %fd508;
	ld.global.nc.f64 	%fd510, [%rd323+48];
	fma.rn.f64 	%fd79, %fd509, %fd78, %fd510;
	fma.rn.f64 	%fd860, %fd79, %fd858, %fd858;
	@%p53 bra 	$L__BB0_79;

	mov.f64 	%fd511, 0d3FF0000000000000;
	fma.rn.f64 	%fd860, %fd79, %fd78, %fd511;

$L__BB0_79:
	and.b32  	%r263, %r56, 2;
	setp.eq.s32 	%p54, %r263, 0;
	@%p54 bra 	$L__BB0_81;

	mov.f64 	%fd512, 0d0000000000000000;
	mov.f64 	%fd513, 0dBFF0000000000000;
	fma.rn.f64 	%fd860, %fd860, %fd513, %fd512;

$L__BB0_81:
	mul.f64 	%fd861, %fd22, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r264, %temp}, %fd861;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r374}, %fd861;
	}
	and.b32  	%r265, %r374, 2147483647;
	setp.ne.s32 	%p55, %r265, 2146435072;
	setp.ne.s32 	%p56, %r264, 0;
	or.pred  	%p57, %p56, %p55;
	@%p57 bra 	$L__BB0_83;

	mov.f64 	%fd514, 0d0000000000000000;
	mul.rn.f64 	%fd861, %fd861, %fd514;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r374}, %fd861;
	}

$L__BB0_83:
	mul.f64 	%fd515, %fd861, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r375, %fd515;
	st.local.u32 	[%rd10], %r375;
	cvt.rn.f64.s32 	%fd516, %r375;
	neg.f64 	%fd517, %fd516;
	fma.rn.f64 	%fd519, %fd517, %fd421, %fd861;
	fma.rn.f64 	%fd521, %fd517, %fd423, %fd519;
	fma.rn.f64 	%fd862, %fd517, %fd425, %fd521;
	and.b32  	%r266, %r374, 2145386496;
	setp.lt.u32 	%p58, %r266, 1105199104;
	@%p58 bra 	$L__BB0_85;

	{ // callseq 30, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd861;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd862, [retval0+0];
	} // callseq 30
	ld.local.u32 	%r375, [%rd10];

$L__BB0_85:
	add.s32 	%r63, %r375, 1;
	and.b32  	%r267, %r63, 1;
	shl.b32 	%r268, %r63, 3;
	and.b32  	%r269, %r268, 8;
	setp.eq.s32 	%p59, %r267, 0;
	selp.f64 	%fd523, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p59;
	mul.wide.s32 	%rd325, %r269, 8;
	add.s64 	%rd327, %rd307, %rd325;
	ld.global.nc.f64 	%fd524, [%rd327+8];
	mul.rn.f64 	%fd91, %fd862, %fd862;
	fma.rn.f64 	%fd525, %fd523, %fd91, %fd524;
	ld.global.nc.f64 	%fd526, [%rd327+16];
	fma.rn.f64 	%fd527, %fd525, %fd91, %fd526;
	ld.global.nc.f64 	%fd528, [%rd327+24];
	fma.rn.f64 	%fd529, %fd527, %fd91, %fd528;
	ld.global.nc.f64 	%fd530, [%rd327+32];
	fma.rn.f64 	%fd531, %fd529, %fd91, %fd530;
	ld.global.nc.f64 	%fd532, [%rd327+40];
	fma.rn.f64 	%fd533, %fd531, %fd91, %fd532;
	ld.global.nc.f64 	%fd534, [%rd327+48];
	fma.rn.f64 	%fd92, %fd533, %fd91, %fd534;
	fma.rn.f64 	%fd864, %fd92, %fd862, %fd862;
	@%p59 bra 	$L__BB0_87;

	mov.f64 	%fd535, 0d3FF0000000000000;
	fma.rn.f64 	%fd864, %fd92, %fd91, %fd535;

$L__BB0_87:
	and.b32  	%r270, %r63, 2;
	setp.eq.s32 	%p60, %r270, 0;
	@%p60 bra 	$L__BB0_89;

	mov.f64 	%fd536, 0d0000000000000000;
	mov.f64 	%fd537, 0dBFF0000000000000;
	fma.rn.f64 	%fd864, %fd864, %fd537, %fd536;

$L__BB0_89:
	mul.f64 	%fd865, %fd23, %fd57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r271, %temp}, %fd865;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r376}, %fd865;
	}
	and.b32  	%r272, %r376, 2147483647;
	setp.ne.s32 	%p61, %r272, 2146435072;
	setp.ne.s32 	%p62, %r271, 0;
	or.pred  	%p63, %p62, %p61;
	@%p63 bra 	$L__BB0_91;

	mov.f64 	%fd538, 0d0000000000000000;
	mul.rn.f64 	%fd865, %fd865, %fd538;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r376}, %fd865;
	}

$L__BB0_91:
	mul.f64 	%fd539, %fd865, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r377, %fd539;
	st.local.u32 	[%rd10], %r377;
	cvt.rn.f64.s32 	%fd540, %r377;
	neg.f64 	%fd541, %fd540;
	fma.rn.f64 	%fd543, %fd541, %fd421, %fd865;
	fma.rn.f64 	%fd545, %fd541, %fd423, %fd543;
	fma.rn.f64 	%fd866, %fd541, %fd425, %fd545;
	and.b32  	%r273, %r376, 2145386496;
	setp.lt.u32 	%p64, %r273, 1105199104;
	mul.f64 	%fd102, %fd860, %fd864;
	@%p64 bra 	$L__BB0_93;

	{ // callseq 31, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd865;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd866, [retval0+0];
	} // callseq 31
	ld.local.u32 	%r377, [%rd10];

$L__BB0_93:
	add.s32 	%r70, %r377, 1;
	and.b32  	%r274, %r70, 1;
	shl.b32 	%r275, %r70, 3;
	and.b32  	%r276, %r275, 8;
	setp.eq.s32 	%p65, %r274, 0;
	selp.f64 	%fd547, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p65;
	mul.wide.s32 	%rd329, %r276, 8;
	add.s64 	%rd331, %rd307, %rd329;
	ld.global.nc.f64 	%fd548, [%rd331+8];
	mul.rn.f64 	%fd105, %fd866, %fd866;
	fma.rn.f64 	%fd549, %fd547, %fd105, %fd548;
	ld.global.nc.f64 	%fd550, [%rd331+16];
	fma.rn.f64 	%fd551, %fd549, %fd105, %fd550;
	ld.global.nc.f64 	%fd552, [%rd331+24];
	fma.rn.f64 	%fd553, %fd551, %fd105, %fd552;
	ld.global.nc.f64 	%fd554, [%rd331+32];
	fma.rn.f64 	%fd555, %fd553, %fd105, %fd554;
	ld.global.nc.f64 	%fd556, [%rd331+40];
	fma.rn.f64 	%fd557, %fd555, %fd105, %fd556;
	ld.global.nc.f64 	%fd558, [%rd331+48];
	fma.rn.f64 	%fd106, %fd557, %fd105, %fd558;
	fma.rn.f64 	%fd868, %fd106, %fd866, %fd866;
	@%p65 bra 	$L__BB0_95;

	mov.f64 	%fd559, 0d3FF0000000000000;
	fma.rn.f64 	%fd868, %fd106, %fd105, %fd559;

$L__BB0_95:
	and.b32  	%r277, %r70, 2;
	setp.eq.s32 	%p66, %r277, 0;
	@%p66 bra 	$L__BB0_97;

	mov.f64 	%fd560, 0d0000000000000000;
	mov.f64 	%fd561, 0dBFF0000000000000;
	fma.rn.f64 	%fd868, %fd868, %fd561, %fd560;

$L__BB0_97:
	shl.b32 	%r355, %r365, 2;
	mul.wide.s32 	%rd424, %r355, 4;
	ld.param.u64 	%rd423, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_37];
	add.s64 	%rd422, %rd423, %rd424;
	add.s64 	%rd421, %rd422, 12;
	add.s64 	%rd420, %rd422, 8;
	add.s64 	%rd419, %rd422, 4;
	mov.u64 	%rd418, 0;
	sub.f64 	%fd843, %fd21, %fd18;
	sub.f64 	%fd842, %fd22, %fd19;
	sub.f64 	%fd841, %fd23, %fd20;
	mul.wide.s32 	%rd416, %r362, 8;
	add.s64 	%rd415, %rd35, %rd416;
	sqrt.rn.f64 	%fd563, %fd27;
	shl.b64 	%rd337, %rd80, 3;
	add.s64 	%rd338, %rd17, %rd337;
	mul.f64 	%fd564, %fd102, %fd868;
	st.local.f64 	[%rd338], %fd564;
	ld.local.f64 	%fd565, [%rd17];
	ld.local.f64 	%fd566, [%rd1];
	sub.f64 	%fd567, %fd566, %fd565;
	ld.local.f64 	%fd568, [%rd17+8];
	ld.local.f64 	%fd569, [%rd1+8];
	sub.f64 	%fd570, %fd569, %fd568;
	ld.local.f64 	%fd571, [%rd17+16];
	ld.local.f64 	%fd572, [%rd1+16];
	sub.f64 	%fd573, %fd572, %fd571;
	mul.f64 	%fd574, %fd841, %fd573;
	fma.rn.f64 	%fd575, %fd842, %fd570, %fd574;
	fma.rn.f64 	%fd576, %fd843, %fd567, %fd575;
	div.rn.f64 	%fd577, %fd576, 0d402921FB54442D18;
	mul.f64 	%fd578, %fd563, %fd563;
	mul.f64 	%fd579, %fd563, %fd578;
	div.rn.f64 	%fd580, %fd577, %fd579;
	mul.f64 	%fd581, %fd416, %fd580;
	mul.f64 	%fd582, %fd17, %fd14;
	fma.rn.f64 	%fd583, %fd16, %fd13, %fd582;
	fma.rn.f64 	%fd584, %fd15, %fd12, %fd583;
	mul.f64 	%fd112, %fd581, %fd584;
	// begin inline asm
	ld.global.nc.f64 %fd562, [%rd415];
	// end inline asm
	mov.f64 	%fd585, 0d3FB45F306DC9C883;
	div.rn.f64 	%fd586, %fd585, %fd563;
	mul.f64 	%fd113, %fd562, %fd586;
	// begin inline asm
	ld.global.nc.s32 %r278, [%rd422];
	// end inline asm
	// begin inline asm
	ld.global.nc.s32 %r279, [%rd419];
	// end inline asm
	// begin inline asm
	ld.global.nc.s32 %r280, [%rd420];
	// end inline asm
	// begin inline asm
	ld.global.nc.s32 %r281, [%rd421];
	// end inline asm
	st.local.u64 	[%rd24], %rd418;
	st.local.u64 	[%rd24+8], %rd418;
	st.local.u64 	[%rd24+16], %rd418;
	st.local.u64 	[%rd24+24], %rd418;
	st.local.u64 	[%rd24+32], %rd418;
	st.local.u64 	[%rd24+40], %rd418;
	st.local.u64 	[%rd24+48], %rd418;
	st.local.u64 	[%rd24+56], %rd418;
	st.local.u64 	[%rd24+64], %rd418;
	cvt.rn.f64.s32 	%fd114, %r278;
	mul.f64 	%fd893, %fd21, %fd114;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r282, %temp}, %fd893;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r390}, %fd893;
	}
	and.b32  	%r283, %r390, 2147483647;
	setp.eq.s32 	%p67, %r283, 2146435072;
	setp.eq.s32 	%p68, %r282, 0;
	and.pred  	%p1, %p68, %p67;
	not.pred 	%p69, %p1;
	mov.u32 	%r378, %r390;
	mov.f64 	%fd869, %fd893;
	@%p69 bra 	$L__BB0_99;

	mov.f64 	%fd587, 0d0000000000000000;
	mul.rn.f64 	%fd869, %fd893, %fd587;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r378}, %fd869;
	}

$L__BB0_99:
	mul.f64 	%fd588, %fd869, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r379, %fd588;
	st.local.u32 	[%rd1], %r379;
	cvt.rn.f64.s32 	%fd589, %r379;
	neg.f64 	%fd590, %fd589;
	fma.rn.f64 	%fd592, %fd590, %fd421, %fd869;
	fma.rn.f64 	%fd594, %fd590, %fd423, %fd592;
	fma.rn.f64 	%fd870, %fd590, %fd425, %fd594;
	and.b32  	%r284, %r378, 2145386496;
	setp.lt.u32 	%p70, %r284, 1105199104;
	@%p70 bra 	$L__BB0_101;

	{ // callseq 32, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd869;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd870, [retval0+0];
	} // callseq 32
	ld.local.u32 	%r379, [%rd1];

$L__BB0_101:
	and.b32  	%r285, %r379, 1;
	shl.b32 	%r286, %r379, 3;
	and.b32  	%r287, %r286, 8;
	setp.eq.s32 	%p71, %r285, 0;
	selp.f64 	%fd596, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p71;
	mul.wide.s32 	%rd341, %r287, 8;
	add.s64 	%rd343, %rd307, %rd341;
	ld.global.nc.f64 	%fd597, [%rd343+8];
	mul.rn.f64 	%fd121, %fd870, %fd870;
	fma.rn.f64 	%fd598, %fd596, %fd121, %fd597;
	ld.global.nc.f64 	%fd599, [%rd343+16];
	fma.rn.f64 	%fd600, %fd598, %fd121, %fd599;
	ld.global.nc.f64 	%fd601, [%rd343+24];
	fma.rn.f64 	%fd602, %fd600, %fd121, %fd601;
	ld.global.nc.f64 	%fd603, [%rd343+32];
	fma.rn.f64 	%fd604, %fd602, %fd121, %fd603;
	ld.global.nc.f64 	%fd605, [%rd343+40];
	fma.rn.f64 	%fd606, %fd604, %fd121, %fd605;
	ld.global.nc.f64 	%fd607, [%rd343+48];
	fma.rn.f64 	%fd122, %fd606, %fd121, %fd607;
	fma.rn.f64 	%fd872, %fd122, %fd870, %fd870;
	@%p71 bra 	$L__BB0_103;

	mov.f64 	%fd608, 0d3FF0000000000000;
	fma.rn.f64 	%fd872, %fd122, %fd121, %fd608;

$L__BB0_103:
	and.b32  	%r288, %r379, 2;
	setp.eq.s32 	%p72, %r288, 0;
	@%p72 bra 	$L__BB0_105;

	mov.f64 	%fd609, 0d0000000000000000;
	mov.f64 	%fd610, 0dBFF0000000000000;
	fma.rn.f64 	%fd872, %fd872, %fd610, %fd609;

$L__BB0_105:
	cvt.rn.f64.s32 	%fd844, %r278;
	mul.f64 	%fd128, %fd872, %fd844;
	cvt.rn.f64.s32 	%fd129, %r279;
	mul.f64 	%fd897, %fd22, %fd129;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r289, %temp}, %fd897;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd897;
	}
	and.b32  	%r290, %r392, 2147483647;
	setp.eq.s32 	%p73, %r290, 2146435072;
	setp.eq.s32 	%p74, %r289, 0;
	and.pred  	%p2, %p74, %p73;
	not.pred 	%p75, %p2;
	mov.u32 	%r380, %r392;
	mov.f64 	%fd873, %fd897;
	@%p75 bra 	$L__BB0_107;

	mov.f64 	%fd611, 0d0000000000000000;
	mul.rn.f64 	%fd873, %fd897, %fd611;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r380}, %fd873;
	}

$L__BB0_107:
	mul.f64 	%fd612, %fd873, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r381, %fd612;
	st.local.u32 	[%rd1], %r381;
	cvt.rn.f64.s32 	%fd613, %r381;
	neg.f64 	%fd614, %fd613;
	fma.rn.f64 	%fd616, %fd614, %fd421, %fd873;
	fma.rn.f64 	%fd618, %fd614, %fd423, %fd616;
	fma.rn.f64 	%fd874, %fd614, %fd425, %fd618;
	and.b32  	%r291, %r380, 2145386496;
	setp.lt.u32 	%p76, %r291, 1105199104;
	@%p76 bra 	$L__BB0_109;

	{ // callseq 33, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd873;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd874, [retval0+0];
	} // callseq 33
	ld.local.u32 	%r381, [%rd1];

$L__BB0_109:
	add.s32 	%r86, %r381, 1;
	and.b32  	%r292, %r86, 1;
	shl.b32 	%r293, %r86, 3;
	and.b32  	%r294, %r293, 8;
	setp.eq.s32 	%p77, %r292, 0;
	selp.f64 	%fd620, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p77;
	mul.wide.s32 	%rd345, %r294, 8;
	add.s64 	%rd347, %rd307, %rd345;
	ld.global.nc.f64 	%fd621, [%rd347+8];
	mul.rn.f64 	%fd136, %fd874, %fd874;
	fma.rn.f64 	%fd622, %fd620, %fd136, %fd621;
	ld.global.nc.f64 	%fd623, [%rd347+16];
	fma.rn.f64 	%fd624, %fd622, %fd136, %fd623;
	ld.global.nc.f64 	%fd625, [%rd347+24];
	fma.rn.f64 	%fd626, %fd624, %fd136, %fd625;
	ld.global.nc.f64 	%fd627, [%rd347+32];
	fma.rn.f64 	%fd628, %fd626, %fd136, %fd627;
	ld.global.nc.f64 	%fd629, [%rd347+40];
	fma.rn.f64 	%fd630, %fd628, %fd136, %fd629;
	ld.global.nc.f64 	%fd631, [%rd347+48];
	fma.rn.f64 	%fd137, %fd630, %fd136, %fd631;
	fma.rn.f64 	%fd876, %fd137, %fd874, %fd874;
	@%p77 bra 	$L__BB0_111;

	mov.f64 	%fd632, 0d3FF0000000000000;
	fma.rn.f64 	%fd876, %fd137, %fd136, %fd632;

$L__BB0_111:
	and.b32  	%r295, %r86, 2;
	setp.eq.s32 	%p78, %r295, 0;
	@%p78 bra 	$L__BB0_113;

	mov.f64 	%fd633, 0d0000000000000000;
	mov.f64 	%fd634, 0dBFF0000000000000;
	fma.rn.f64 	%fd876, %fd876, %fd634, %fd633;

$L__BB0_113:
	mul.f64 	%fd143, %fd128, %fd876;
	cvt.rn.f64.s32 	%fd144, %r280;
	mul.f64 	%fd901, %fd23, %fd144;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r296, %temp}, %fd901;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd901;
	}
	and.b32  	%r297, %r394, 2147483647;
	setp.eq.s32 	%p79, %r297, 2146435072;
	setp.eq.s32 	%p80, %r296, 0;
	and.pred  	%p3, %p80, %p79;
	not.pred 	%p81, %p3;
	mov.u32 	%r382, %r394;
	mov.f64 	%fd877, %fd901;
	@%p81 bra 	$L__BB0_115;

	mov.f64 	%fd635, 0d0000000000000000;
	mul.rn.f64 	%fd877, %fd901, %fd635;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r382}, %fd877;
	}

$L__BB0_115:
	mul.f64 	%fd636, %fd877, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r383, %fd636;
	st.local.u32 	[%rd1], %r383;
	cvt.rn.f64.s32 	%fd637, %r383;
	neg.f64 	%fd638, %fd637;
	fma.rn.f64 	%fd640, %fd638, %fd421, %fd877;
	fma.rn.f64 	%fd642, %fd638, %fd423, %fd640;
	fma.rn.f64 	%fd878, %fd638, %fd425, %fd642;
	and.b32  	%r298, %r382, 2145386496;
	setp.lt.u32 	%p82, %r298, 1105199104;
	@%p82 bra 	$L__BB0_117;

	{ // callseq 34, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd877;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd878, [retval0+0];
	} // callseq 34
	ld.local.u32 	%r383, [%rd1];

$L__BB0_117:
	add.s32 	%r93, %r383, 1;
	and.b32  	%r299, %r93, 1;
	shl.b32 	%r300, %r93, 3;
	and.b32  	%r301, %r300, 8;
	setp.eq.s32 	%p83, %r299, 0;
	selp.f64 	%fd644, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p83;
	mul.wide.s32 	%rd349, %r301, 8;
	add.s64 	%rd351, %rd307, %rd349;
	ld.global.nc.f64 	%fd645, [%rd351+8];
	mul.rn.f64 	%fd151, %fd878, %fd878;
	fma.rn.f64 	%fd646, %fd644, %fd151, %fd645;
	ld.global.nc.f64 	%fd647, [%rd351+16];
	fma.rn.f64 	%fd648, %fd646, %fd151, %fd647;
	ld.global.nc.f64 	%fd649, [%rd351+24];
	fma.rn.f64 	%fd650, %fd648, %fd151, %fd649;
	ld.global.nc.f64 	%fd651, [%rd351+32];
	fma.rn.f64 	%fd652, %fd650, %fd151, %fd651;
	ld.global.nc.f64 	%fd653, [%rd351+40];
	fma.rn.f64 	%fd654, %fd652, %fd151, %fd653;
	ld.global.nc.f64 	%fd655, [%rd351+48];
	fma.rn.f64 	%fd152, %fd654, %fd151, %fd655;
	fma.rn.f64 	%fd880, %fd152, %fd878, %fd878;
	@%p83 bra 	$L__BB0_119;

	mov.f64 	%fd656, 0d3FF0000000000000;
	fma.rn.f64 	%fd880, %fd152, %fd151, %fd656;

$L__BB0_119:
	and.b32  	%r302, %r93, 2;
	setp.eq.s32 	%p84, %r302, 0;
	@%p84 bra 	$L__BB0_121;

	mov.f64 	%fd657, 0d0000000000000000;
	mov.f64 	%fd658, 0dBFF0000000000000;
	fma.rn.f64 	%fd880, %fd880, %fd658, %fd657;

$L__BB0_121:
	mul.f64 	%fd158, %fd143, %fd880;
	mov.u32 	%r384, %r390;
	mov.f64 	%fd881, %fd893;
	@%p69 bra 	$L__BB0_123;

	mov.f64 	%fd659, 0d0000000000000000;
	mul.rn.f64 	%fd881, %fd893, %fd659;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r384}, %fd881;
	}

$L__BB0_123:
	mul.f64 	%fd660, %fd881, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r385, %fd660;
	st.local.u32 	[%rd1], %r385;
	cvt.rn.f64.s32 	%fd661, %r385;
	neg.f64 	%fd662, %fd661;
	fma.rn.f64 	%fd664, %fd662, %fd421, %fd881;
	fma.rn.f64 	%fd666, %fd662, %fd423, %fd664;
	fma.rn.f64 	%fd882, %fd662, %fd425, %fd666;
	and.b32  	%r303, %r384, 2145386496;
	setp.lt.u32 	%p86, %r303, 1105199104;
	@%p86 bra 	$L__BB0_125;

	{ // callseq 35, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd881;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd882, [retval0+0];
	} // callseq 35
	ld.local.u32 	%r385, [%rd1];

$L__BB0_125:
	add.s32 	%r99, %r385, 1;
	and.b32  	%r304, %r99, 1;
	shl.b32 	%r305, %r99, 3;
	and.b32  	%r306, %r305, 8;
	setp.eq.s32 	%p87, %r304, 0;
	selp.f64 	%fd668, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p87;
	mul.wide.s32 	%rd353, %r306, 8;
	add.s64 	%rd355, %rd307, %rd353;
	ld.global.nc.f64 	%fd669, [%rd355+8];
	mul.rn.f64 	%fd164, %fd882, %fd882;
	fma.rn.f64 	%fd670, %fd668, %fd164, %fd669;
	ld.global.nc.f64 	%fd671, [%rd355+16];
	fma.rn.f64 	%fd672, %fd670, %fd164, %fd671;
	ld.global.nc.f64 	%fd673, [%rd355+24];
	fma.rn.f64 	%fd674, %fd672, %fd164, %fd673;
	ld.global.nc.f64 	%fd675, [%rd355+32];
	fma.rn.f64 	%fd676, %fd674, %fd164, %fd675;
	ld.global.nc.f64 	%fd677, [%rd355+40];
	fma.rn.f64 	%fd678, %fd676, %fd164, %fd677;
	ld.global.nc.f64 	%fd679, [%rd355+48];
	fma.rn.f64 	%fd165, %fd678, %fd164, %fd679;
	fma.rn.f64 	%fd884, %fd165, %fd882, %fd882;
	@%p87 bra 	$L__BB0_127;

	mov.f64 	%fd680, 0d3FF0000000000000;
	fma.rn.f64 	%fd884, %fd165, %fd164, %fd680;

$L__BB0_127:
	and.b32  	%r307, %r99, 2;
	setp.eq.s32 	%p88, %r307, 0;
	@%p88 bra 	$L__BB0_129;

	mov.f64 	%fd681, 0d0000000000000000;
	mov.f64 	%fd682, 0dBFF0000000000000;
	fma.rn.f64 	%fd884, %fd884, %fd682, %fd681;

$L__BB0_129:
	cvt.rn.f64.s32 	%fd837, %r279;
	mul.f64 	%fd171, %fd884, %fd837;
	mov.u32 	%r386, %r392;
	mov.f64 	%fd885, %fd897;
	@%p75 bra 	$L__BB0_131;

	mov.f64 	%fd683, 0d0000000000000000;
	mul.rn.f64 	%fd885, %fd897, %fd683;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r386}, %fd885;
	}

$L__BB0_131:
	mul.f64 	%fd684, %fd885, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r387, %fd684;
	st.local.u32 	[%rd1], %r387;
	cvt.rn.f64.s32 	%fd685, %r387;
	neg.f64 	%fd686, %fd685;
	fma.rn.f64 	%fd688, %fd686, %fd421, %fd885;
	fma.rn.f64 	%fd690, %fd686, %fd423, %fd688;
	fma.rn.f64 	%fd886, %fd686, %fd425, %fd690;
	and.b32  	%r308, %r386, 2145386496;
	setp.lt.u32 	%p90, %r308, 1105199104;
	@%p90 bra 	$L__BB0_133;

	{ // callseq 36, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd885;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd886, [retval0+0];
	} // callseq 36
	ld.local.u32 	%r387, [%rd1];

$L__BB0_133:
	and.b32  	%r309, %r387, 1;
	shl.b32 	%r310, %r387, 3;
	and.b32  	%r311, %r310, 8;
	setp.eq.s32 	%p91, %r309, 0;
	selp.f64 	%fd692, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p91;
	mul.wide.s32 	%rd357, %r311, 8;
	add.s64 	%rd359, %rd307, %rd357;
	ld.global.nc.f64 	%fd693, [%rd359+8];
	mul.rn.f64 	%fd177, %fd886, %fd886;
	fma.rn.f64 	%fd694, %fd692, %fd177, %fd693;
	ld.global.nc.f64 	%fd695, [%rd359+16];
	fma.rn.f64 	%fd696, %fd694, %fd177, %fd695;
	ld.global.nc.f64 	%fd697, [%rd359+24];
	fma.rn.f64 	%fd698, %fd696, %fd177, %fd697;
	ld.global.nc.f64 	%fd699, [%rd359+32];
	fma.rn.f64 	%fd700, %fd698, %fd177, %fd699;
	ld.global.nc.f64 	%fd701, [%rd359+40];
	fma.rn.f64 	%fd702, %fd700, %fd177, %fd701;
	ld.global.nc.f64 	%fd703, [%rd359+48];
	fma.rn.f64 	%fd178, %fd702, %fd177, %fd703;
	fma.rn.f64 	%fd888, %fd178, %fd886, %fd886;
	@%p91 bra 	$L__BB0_135;

	mov.f64 	%fd704, 0d3FF0000000000000;
	fma.rn.f64 	%fd888, %fd178, %fd177, %fd704;

$L__BB0_135:
	and.b32  	%r312, %r387, 2;
	setp.eq.s32 	%p92, %r312, 0;
	@%p92 bra 	$L__BB0_137;

	mov.f64 	%fd705, 0d0000000000000000;
	mov.f64 	%fd706, 0dBFF0000000000000;
	fma.rn.f64 	%fd888, %fd888, %fd706, %fd705;

$L__BB0_137:
	mul.f64 	%fd184, %fd171, %fd888;
	mov.u32 	%r388, %r394;
	mov.f64 	%fd889, %fd901;
	@%p81 bra 	$L__BB0_139;

	mov.f64 	%fd707, 0d0000000000000000;
	mul.rn.f64 	%fd889, %fd901, %fd707;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r388}, %fd889;
	}

$L__BB0_139:
	mul.f64 	%fd708, %fd889, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r389, %fd708;
	st.local.u32 	[%rd1], %r389;
	cvt.rn.f64.s32 	%fd709, %r389;
	neg.f64 	%fd710, %fd709;
	fma.rn.f64 	%fd712, %fd710, %fd421, %fd889;
	fma.rn.f64 	%fd714, %fd710, %fd423, %fd712;
	fma.rn.f64 	%fd890, %fd710, %fd425, %fd714;
	and.b32  	%r313, %r388, 2145386496;
	setp.lt.u32 	%p94, %r313, 1105199104;
	@%p94 bra 	$L__BB0_141;

	{ // callseq 37, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd889;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd890, [retval0+0];
	} // callseq 37
	ld.local.u32 	%r389, [%rd1];

$L__BB0_141:
	add.s32 	%r110, %r389, 1;
	and.b32  	%r314, %r110, 1;
	shl.b32 	%r315, %r110, 3;
	and.b32  	%r316, %r315, 8;
	setp.eq.s32 	%p95, %r314, 0;
	selp.f64 	%fd716, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p95;
	mul.wide.s32 	%rd361, %r316, 8;
	add.s64 	%rd363, %rd307, %rd361;
	ld.global.nc.f64 	%fd717, [%rd363+8];
	mul.rn.f64 	%fd190, %fd890, %fd890;
	fma.rn.f64 	%fd718, %fd716, %fd190, %fd717;
	ld.global.nc.f64 	%fd719, [%rd363+16];
	fma.rn.f64 	%fd720, %fd718, %fd190, %fd719;
	ld.global.nc.f64 	%fd721, [%rd363+24];
	fma.rn.f64 	%fd722, %fd720, %fd190, %fd721;
	ld.global.nc.f64 	%fd723, [%rd363+32];
	fma.rn.f64 	%fd724, %fd722, %fd190, %fd723;
	ld.global.nc.f64 	%fd725, [%rd363+40];
	fma.rn.f64 	%fd726, %fd724, %fd190, %fd725;
	ld.global.nc.f64 	%fd727, [%rd363+48];
	fma.rn.f64 	%fd191, %fd726, %fd190, %fd727;
	fma.rn.f64 	%fd892, %fd191, %fd890, %fd890;
	@%p95 bra 	$L__BB0_143;

	mov.f64 	%fd728, 0d3FF0000000000000;
	fma.rn.f64 	%fd892, %fd191, %fd190, %fd728;

$L__BB0_143:
	and.b32  	%r317, %r110, 2;
	setp.eq.s32 	%p96, %r317, 0;
	@%p96 bra 	$L__BB0_145;

	mov.f64 	%fd729, 0d0000000000000000;
	mov.f64 	%fd730, 0dBFF0000000000000;
	fma.rn.f64 	%fd892, %fd892, %fd730, %fd729;

$L__BB0_145:
	mul.f64 	%fd197, %fd184, %fd892;
	@%p69 bra 	$L__BB0_147;

	mov.f64 	%fd731, 0d0000000000000000;
	mul.rn.f64 	%fd893, %fd893, %fd731;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r390}, %fd893;
	}

$L__BB0_147:
	mul.f64 	%fd732, %fd893, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r391, %fd732;
	st.local.u32 	[%rd1], %r391;
	cvt.rn.f64.s32 	%fd733, %r391;
	neg.f64 	%fd734, %fd733;
	fma.rn.f64 	%fd736, %fd734, %fd421, %fd893;
	fma.rn.f64 	%fd738, %fd734, %fd423, %fd736;
	fma.rn.f64 	%fd894, %fd734, %fd425, %fd738;
	and.b32  	%r318, %r390, 2145386496;
	setp.lt.u32 	%p98, %r318, 1105199104;
	@%p98 bra 	$L__BB0_149;

	{ // callseq 38, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd893;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd894, [retval0+0];
	} // callseq 38
	ld.local.u32 	%r391, [%rd1];

$L__BB0_149:
	add.s32 	%r116, %r391, 1;
	and.b32  	%r319, %r116, 1;
	shl.b32 	%r320, %r116, 3;
	and.b32  	%r321, %r320, 8;
	setp.eq.s32 	%p99, %r319, 0;
	selp.f64 	%fd740, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p99;
	mul.wide.s32 	%rd365, %r321, 8;
	add.s64 	%rd367, %rd307, %rd365;
	ld.global.nc.f64 	%fd741, [%rd367+8];
	mul.rn.f64 	%fd203, %fd894, %fd894;
	fma.rn.f64 	%fd742, %fd740, %fd203, %fd741;
	ld.global.nc.f64 	%fd743, [%rd367+16];
	fma.rn.f64 	%fd744, %fd742, %fd203, %fd743;
	ld.global.nc.f64 	%fd745, [%rd367+24];
	fma.rn.f64 	%fd746, %fd744, %fd203, %fd745;
	ld.global.nc.f64 	%fd747, [%rd367+32];
	fma.rn.f64 	%fd748, %fd746, %fd203, %fd747;
	ld.global.nc.f64 	%fd749, [%rd367+40];
	fma.rn.f64 	%fd750, %fd748, %fd203, %fd749;
	ld.global.nc.f64 	%fd751, [%rd367+48];
	fma.rn.f64 	%fd204, %fd750, %fd203, %fd751;
	fma.rn.f64 	%fd896, %fd204, %fd894, %fd894;
	@%p99 bra 	$L__BB0_151;

	mov.f64 	%fd752, 0d3FF0000000000000;
	fma.rn.f64 	%fd896, %fd204, %fd203, %fd752;

$L__BB0_151:
	and.b32  	%r322, %r116, 2;
	setp.eq.s32 	%p100, %r322, 0;
	@%p100 bra 	$L__BB0_153;

	mov.f64 	%fd753, 0d0000000000000000;
	mov.f64 	%fd754, 0dBFF0000000000000;
	fma.rn.f64 	%fd896, %fd896, %fd754, %fd753;

$L__BB0_153:
	cvt.rn.f64.s32 	%fd838, %r280;
	mul.f64 	%fd210, %fd896, %fd838;
	@%p75 bra 	$L__BB0_155;

	mov.f64 	%fd755, 0d0000000000000000;
	mul.rn.f64 	%fd897, %fd897, %fd755;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd897;
	}

$L__BB0_155:
	mul.f64 	%fd756, %fd897, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r393, %fd756;
	st.local.u32 	[%rd1], %r393;
	cvt.rn.f64.s32 	%fd757, %r393;
	neg.f64 	%fd758, %fd757;
	fma.rn.f64 	%fd760, %fd758, %fd421, %fd897;
	fma.rn.f64 	%fd762, %fd758, %fd423, %fd760;
	fma.rn.f64 	%fd898, %fd758, %fd425, %fd762;
	and.b32  	%r323, %r392, 2145386496;
	setp.lt.u32 	%p102, %r323, 1105199104;
	@%p102 bra 	$L__BB0_157;

	{ // callseq 39, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd897;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd898, [retval0+0];
	} // callseq 39
	ld.local.u32 	%r393, [%rd1];

$L__BB0_157:
	add.s32 	%r122, %r393, 1;
	and.b32  	%r324, %r122, 1;
	shl.b32 	%r325, %r122, 3;
	and.b32  	%r326, %r325, 8;
	setp.eq.s32 	%p103, %r324, 0;
	selp.f64 	%fd764, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p103;
	mul.wide.s32 	%rd369, %r326, 8;
	add.s64 	%rd371, %rd307, %rd369;
	ld.global.nc.f64 	%fd765, [%rd371+8];
	mul.rn.f64 	%fd216, %fd898, %fd898;
	fma.rn.f64 	%fd766, %fd764, %fd216, %fd765;
	ld.global.nc.f64 	%fd767, [%rd371+16];
	fma.rn.f64 	%fd768, %fd766, %fd216, %fd767;
	ld.global.nc.f64 	%fd769, [%rd371+24];
	fma.rn.f64 	%fd770, %fd768, %fd216, %fd769;
	ld.global.nc.f64 	%fd771, [%rd371+32];
	fma.rn.f64 	%fd772, %fd770, %fd216, %fd771;
	ld.global.nc.f64 	%fd773, [%rd371+40];
	fma.rn.f64 	%fd774, %fd772, %fd216, %fd773;
	ld.global.nc.f64 	%fd775, [%rd371+48];
	fma.rn.f64 	%fd217, %fd774, %fd216, %fd775;
	fma.rn.f64 	%fd900, %fd217, %fd898, %fd898;
	@%p103 bra 	$L__BB0_159;

	mov.f64 	%fd776, 0d3FF0000000000000;
	fma.rn.f64 	%fd900, %fd217, %fd216, %fd776;

$L__BB0_159:
	and.b32  	%r327, %r122, 2;
	setp.eq.s32 	%p104, %r327, 0;
	@%p104 bra 	$L__BB0_161;

	mov.f64 	%fd777, 0d0000000000000000;
	mov.f64 	%fd778, 0dBFF0000000000000;
	fma.rn.f64 	%fd900, %fd900, %fd778, %fd777;

$L__BB0_161:
	mul.f64 	%fd223, %fd210, %fd900;
	@%p81 bra 	$L__BB0_163;

	mov.f64 	%fd779, 0d0000000000000000;
	mul.rn.f64 	%fd901, %fd901, %fd779;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd901;
	}

$L__BB0_163:
	mul.f64 	%fd780, %fd901, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r395, %fd780;
	st.local.u32 	[%rd1], %r395;
	cvt.rn.f64.s32 	%fd781, %r395;
	neg.f64 	%fd782, %fd781;
	fma.rn.f64 	%fd784, %fd782, %fd421, %fd901;
	fma.rn.f64 	%fd786, %fd782, %fd423, %fd784;
	fma.rn.f64 	%fd902, %fd782, %fd425, %fd786;
	and.b32  	%r328, %r394, 2145386496;
	setp.lt.u32 	%p106, %r328, 1105199104;
	@%p106 bra 	$L__BB0_165;

	{ // callseq 40, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd901;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd110;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd902, [retval0+0];
	} // callseq 40
	ld.local.u32 	%r395, [%rd1];

$L__BB0_165:
	and.b32  	%r329, %r395, 1;
	shl.b32 	%r330, %r395, 3;
	and.b32  	%r331, %r330, 8;
	setp.eq.s32 	%p107, %r329, 0;
	selp.f64 	%fd788, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p107;
	mul.wide.s32 	%rd373, %r331, 8;
	add.s64 	%rd375, %rd307, %rd373;
	ld.global.nc.f64 	%fd789, [%rd375+8];
	mul.rn.f64 	%fd229, %fd902, %fd902;
	fma.rn.f64 	%fd790, %fd788, %fd229, %fd789;
	ld.global.nc.f64 	%fd791, [%rd375+16];
	fma.rn.f64 	%fd792, %fd790, %fd229, %fd791;
	ld.global.nc.f64 	%fd793, [%rd375+24];
	fma.rn.f64 	%fd794, %fd792, %fd229, %fd793;
	ld.global.nc.f64 	%fd795, [%rd375+32];
	fma.rn.f64 	%fd796, %fd794, %fd229, %fd795;
	ld.global.nc.f64 	%fd797, [%rd375+40];
	fma.rn.f64 	%fd798, %fd796, %fd229, %fd797;
	ld.global.nc.f64 	%fd799, [%rd375+48];
	fma.rn.f64 	%fd230, %fd798, %fd229, %fd799;
	fma.rn.f64 	%fd904, %fd230, %fd902, %fd902;
	@%p107 bra 	$L__BB0_167;

	mov.f64 	%fd800, 0d3FF0000000000000;
	fma.rn.f64 	%fd904, %fd230, %fd229, %fd800;

$L__BB0_167:
	and.b32  	%r332, %r395, 2;
	setp.eq.s32 	%p108, %r332, 0;
	@%p108 bra 	$L__BB0_169;

	mov.f64 	%fd801, 0d0000000000000000;
	mov.f64 	%fd802, 0dBFF0000000000000;
	fma.rn.f64 	%fd904, %fd904, %fd802, %fd801;

$L__BB0_169:
	mul.wide.s32 	%rd376, %r281, 8;
	add.s64 	%rd377, %rd24, %rd376;
	neg.f64 	%fd803, %fd158;
	st.local.f64 	[%rd377], %fd803;
	neg.f64 	%fd804, %fd197;
	st.local.f64 	[%rd377+24], %fd804;
	mul.f64 	%fd805, %fd223, %fd904;
	neg.f64 	%fd806, %fd805;
	st.local.f64 	[%rd377+48], %fd806;
	ld.local.f64 	%fd807, [%rd24];
	ld.local.f64 	%fd808, [%rd24+24];
	ld.local.f64 	%fd809, [%rd24+48];
	mul.f64 	%fd810, %fd809, %fd17;
	fma.rn.f64 	%fd811, %fd808, %fd16, %fd810;
	fma.rn.f64 	%fd812, %fd807, %fd15, %fd811;
	ld.local.f64 	%fd813, [%rd24+8];
	ld.local.f64 	%fd814, [%rd24+32];
	ld.local.f64 	%fd815, [%rd24+56];
	mul.f64 	%fd816, %fd815, %fd17;
	fma.rn.f64 	%fd817, %fd814, %fd16, %fd816;
	fma.rn.f64 	%fd818, %fd813, %fd15, %fd817;
	ld.local.f64 	%fd819, [%rd24+16];
	ld.local.f64 	%fd820, [%rd24+40];
	ld.local.f64 	%fd821, [%rd24+64];
	mul.f64 	%fd822, %fd821, %fd17;
	fma.rn.f64 	%fd823, %fd820, %fd16, %fd822;
	fma.rn.f64 	%fd824, %fd819, %fd15, %fd823;
	mul.f64 	%fd825, %fd824, %fd14;
	fma.rn.f64 	%fd826, %fd818, %fd13, %fd825;
	fma.rn.f64 	%fd827, %fd812, %fd12, %fd826;
	mul.f64 	%fd828, %fd113, %fd827;
	ld.local.u32 	%r333, [%rd76];
	mul.wide.s32 	%rd378, %r333, 8;
	add.s64 	%rd379, %rd27, %rd378;
	fma.rn.f64 	%fd829, %fd112, 0d3FE0000000000000, %fd828;
	ld.global.f64 	%fd830, [%rd379];
	mul.f64 	%fd831, %fd830, %fd829;
	ld.local.u32 	%r334, [%rd77];
	mul.wide.s32 	%rd380, %r334, 8;
	add.s64 	%rd381, %rd27, %rd380;
	ld.global.f64 	%fd832, [%rd381];
	add.s32 	%r335, %r365, %r17;
	shl.b32 	%r336, %r335, 3;
	mov.u32 	%r337, _ZZ22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_E21localShapeDerivatives;
	add.s32 	%r338, %r337, %r336;
	ld.shared.f64 	%fd833, [%r338];
	fma.rn.f64 	%fd834, %fd831, %fd832, %fd833;
	st.shared.f64 	[%r338], %fd834;
	add.s32 	%r365, %r365, 1;
	setp.lt.s32 	%p109, %r365, %r142;
	@%p109 bra 	$L__BB0_49;

$L__BB0_170:
	add.s32 	%r364, %r364, 1;
	setp.lt.u32 	%p110, %r364, 3;
	@%p110 bra 	$L__BB0_47;

	add.s32 	%r363, %r363, 1;
	setp.lt.u32 	%p111, %r363, 3;
	@%p111 bra 	$L__BB0_46;

	add.s32 	%r362, %r362, 1;
	setp.lt.s32 	%p112, %r362, %r20;
	@%p112 bra 	$L__BB0_45;

$L__BB0_173:
	{ // callseq 41, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd64;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 41
	{ // callseq 42, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd57;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 42
	{ // callseq 43, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd56;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 43
	{ // callseq 44, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 44
	{ // callseq 45, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd45;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 45
	{ // callseq 46, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd38;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 46
	{ // callseq 47, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 47
	{ // callseq 48, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd32;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 48
	add.s32 	%r361, %r361, 1;
	setp.lt.s32 	%p113, %r361, %r15;
	@%p113 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_174;

$L__BB0_14:
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 6
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd32;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 7

$L__BB0_174:
	@%p4 bra 	$L__BB0_179;

	ld.param.u64 	%rd412, [_Z22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2__param_23];
	mov.u32 	%r344, %tid.x;
	mul.lo.s32 	%r133, %r344, %r142;
	cvta.to.global.u64 	%rd83, %rd412;
	mov.u32 	%r396, 0;

$L__BB0_176:
	mul.wide.s32 	%rd382, %r396, 8;
	add.s64 	%rd84, %rd83, %rd382;
	add.s32 	%r340, %r396, %r133;
	shl.b32 	%r341, %r340, 3;
	mov.u32 	%r342, _ZZ22computeShapeDerivativeiiiiiiPKtS0_PKiPKdS4_iS4_S4_iS4_S4_iS4_S4_iPdS5_S5_S4_S2_S4_S4_S4_S2_S2_iiiiiiS2_iS2_S2_E21localShapeDerivatives;
	add.s32 	%r343, %r342, %r341;
	ld.shared.f64 	%fd236, [%r343];
	ld.global.u64 	%rd433, [%rd84];

$L__BB0_177:
	mov.b64 	%fd835, %rd433;
	add.f64 	%fd836, %fd236, %fd835;
	mov.b64 	%rd383, %fd836;
	atom.global.cas.b64 	%rd87, [%rd84], %rd433, %rd383;
	setp.ne.s64 	%p115, %rd433, %rd87;
	mov.u64 	%rd433, %rd87;
	@%p115 bra 	$L__BB0_177;

	add.s32 	%r396, %r396, 1;
	setp.lt.s32 	%p116, %r396, %r142;
	@%p116 bra 	$L__BB0_176;

$L__BB0_179:
	ret;

}
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<79>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd18, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd1, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	bfe.u32 	%r2, %r1, 20, 11;
	setp.eq.s32 	%p1, %r2, 2047;
	@%p1 bra 	$L__BB1_7;

	add.s32 	%r3, %r2, -1024;
	shr.u32 	%r10, %r3, 6;
	mov.u32 	%r11, 16;
	sub.s32 	%r4, %r11, %r10;
	mov.u32 	%r12, 19;
	sub.s32 	%r13, %r12, %r10;
	setp.gt.s32 	%p2, %r4, 14;
	selp.b32 	%r5, 18, %r13, %p2;
	setp.gt.s32 	%p3, %r4, %r5;
	mov.u64 	%rd75, 0;
	mov.u64 	%rd76, %rd1;
	@%p3 bra 	$L__BB1_4;

	add.s32 	%r6, %r4, -1;
	mov.b64 	%rd22, %fd4;
	shl.b64 	%rd23, %rd22, 11;
	or.b64  	%rd4, %rd23, -9223372036854775808;
	mov.u64 	%rd25, __cudart_i2opi_d;
	mov.u64 	%rd76, %rd1;
	mov.u32 	%r28, %r6;

$L__BB1_3:
	.pragma "nounroll";
	mul.wide.s32 	%rd24, %r28, 8;
	add.s64 	%rd26, %rd25, %rd24;
	ld.global.nc.u64 	%rd27, [%rd26];
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi, %clo, %chi;
	mov.b64 	{%alo,%ahi}, %rd27;
	mov.b64 	{%blo,%bhi}, %rd4;
	mov.b64 	{%clo,%chi}, %rd75;
	mad.lo.cc.u32 	%r0, %alo, %blo, %clo;
	madc.hi.cc.u32 	%r1, %alo, %blo, %chi;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd28, {%r0,%r1};
	mov.b64 	%rd75, {%r2,%r3};
	}
	st.local.u64 	[%rd76], %rd28;
	add.s32 	%r28, %r28, 1;
	sub.s32 	%r14, %r28, %r6;
	mul.wide.s32 	%rd29, %r14, 8;
	add.s64 	%rd76, %rd1, %rd29;
	setp.lt.s32 	%p4, %r28, %r5;
	@%p4 bra 	$L__BB1_3;

$L__BB1_4:
	st.local.u64 	[%rd76], %rd75;
	ld.local.u64 	%rd78, [%rd1+16];
	ld.local.u64 	%rd77, [%rd1+24];
	and.b32  	%r9, %r3, 63;
	setp.eq.s32 	%p5, %r9, 0;
	@%p5 bra 	$L__BB1_6;

	mov.u32 	%r15, 64;
	sub.s32 	%r16, %r15, %r9;
	shl.b64 	%rd30, %rd77, %r9;
	shr.u64 	%rd31, %rd78, %r16;
	or.b64  	%rd77, %rd30, %rd31;
	shl.b64 	%rd32, %rd78, %r9;
	ld.local.u64 	%rd33, [%rd1+8];
	shr.u64 	%rd34, %rd33, %r16;
	or.b64  	%rd78, %rd34, %rd32;

$L__BB1_6:
	and.b32  	%r17, %r1, -2147483648;
	shr.u64 	%rd35, %rd77, 62;
	cvt.u32.u64 	%r18, %rd35;
	shr.u64 	%rd36, %rd78, 62;
	shl.b64 	%rd37, %rd77, 2;
	or.b64  	%rd38, %rd36, %rd37;
	shr.u64 	%rd39, %rd77, 61;
	cvt.u32.u64 	%r19, %rd39;
	and.b32  	%r20, %r19, 1;
	add.s32 	%r21, %r20, %r18;
	neg.s32 	%r22, %r21;
	setp.eq.s32 	%p6, %r17, 0;
	selp.b32 	%r23, %r21, %r22, %p6;
	cvta.to.local.u64 	%rd40, %rd18;
	mov.u64 	%rd41, 0;
	st.local.u32 	[%rd40], %r23;
	setp.eq.s32 	%p7, %r20, 0;
	shl.b64 	%rd42, %rd78, 2;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd41;
	mov.b64 	{%a2,%a3}, %rd41;
	mov.b64 	{%b0,%b1}, %rd42;
	mov.b64 	{%b2,%b3}, %rd38;
	sub.cc.u32 	%r0, %a0, %b0;
	subc.cc.u32 	%r1, %a1, %b1;
	subc.cc.u32 	%r2, %a2, %b2;
	subc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd43, {%r0,%r1};
	mov.b64 	%rd44, {%r2,%r3};
	}
	selp.b64 	%rd45, %rd38, %rd44, %p7;
	selp.b64 	%rd46, %rd42, %rd43, %p7;
	xor.b32  	%r24, %r17, -2147483648;
	selp.b32 	%r25, %r17, %r24, %p7;
	clz.b64 	%r26, %rd45;
	cvt.u64.u32 	%rd47, %r26;
	setp.eq.s64 	%p8, %rd47, 0;
	shl.b64 	%rd48, %rd45, %r26;
	mov.u64 	%rd49, 64;
	sub.s64 	%rd50, %rd49, %rd47;
	cvt.u32.u64 	%r27, %rd50;
	shr.u64 	%rd51, %rd46, %r27;
	or.b64  	%rd52, %rd51, %rd48;
	selp.b64 	%rd53, %rd45, %rd52, %p8;
	mov.u64 	%rd54, -3958705157555305931;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi;
	mov.b64 	{%alo,%ahi}, %rd53;
	mov.b64 	{%blo,%bhi}, %rd54;
	mul.lo.u32 	%r0, %alo, %blo;
	mul.hi.u32 	%r1, %alo, %blo;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd55, {%r0,%r1};
	mov.b64 	%rd56, {%r2,%r3};
	}
	setp.gt.s64 	%p9, %rd56, 0;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd55;
	mov.b64 	{%a2,%a3}, %rd56;
	mov.b64 	{%b0,%b1}, %rd55;
	mov.b64 	{%b2,%b3}, %rd56;
	add.cc.u32 	%r0, %a0, %b0;
	addc.cc.u32 	%r1, %a1, %b1;
	addc.cc.u32 	%r2, %a2, %b2;
	addc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd57, {%r0,%r1};
	mov.b64 	%rd58, {%r2,%r3};
	}
	selp.b64 	%rd59, %rd58, %rd56, %p9;
	selp.u64 	%rd60, 1, 0, %p9;
	add.s64 	%rd61, %rd47, %rd60;
	cvt.u64.u32 	%rd62, %r25;
	shl.b64 	%rd63, %rd62, 32;
	shl.b64 	%rd64, %rd61, 52;
	mov.u64 	%rd65, 4602678819172646912;
	sub.s64 	%rd66, %rd65, %rd64;
	add.s64 	%rd67, %rd59, 1;
	shr.u64 	%rd68, %rd67, 10;
	add.s64 	%rd69, %rd68, 1;
	shr.u64 	%rd70, %rd69, 1;
	add.s64 	%rd71, %rd66, %rd70;
	or.b64  	%rd72, %rd71, %rd63;
	mov.b64 	%fd4, %rd72;

$L__BB1_7:
	st.param.f64 	[func_retval0+0], %fd4;
	ret;

}

